{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2nd August - Further Exploration of Clustering",
      "provenance": [],
      "collapsed_sections": [
        "svsqrk45tb_B",
        "3eJpHGVpGuji",
        "NmVtQy38Ozr6",
        "uICiUW0YyfAZ",
        "Us6xVpclRb_A",
        "Ly1lLTEs0yxu",
        "hwTqc9CP_DWF",
        "Kky74E1-V67I",
        "snNHe8YWqkk6",
        "o5zYpYf6OAZ7",
        "6PPCqKwxPWQb",
        "BZZeXuJ3VuRn",
        "KTmeCjcSVy5e",
        "ggmITtYHWPOJ",
        "gBwToKIyZTpB",
        "okENxb1objAq",
        "9Za8CMtVgFZx",
        "p2u0ZYwJ517A",
        "qV2Ubwtb8491",
        "1vVEXe71871t",
        "i3b5z8K-9B0K",
        "amkNG_txAlb0",
        "5Nq9hGRqNSfI"
      ],
      "toc_visible": true,
      "mount_file_id": "1XQiLyjIhVdgtFhuzVLHo5QTOzMLa85lV",
      "authorship_tag": "ABX9TyMvpCKVupnOjcAyuJHXHGhn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shengy90/MSc-Project/blob/master/notebooks/2nd_August_Further_Exploration_of_Clustering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svsqrk45tb_B",
        "colab_type": "text"
      },
      "source": [
        "# **1Ô∏è‚É£ Setup Notebook üíª**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eJpHGVpGuji",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### **Authenticate with BigQuery ‚òÅÔ∏è**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkaOt64QmU90",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07e948a6-41e0-4c76-ea44-ef625db86bce"
      },
      "source": [
        "!pip install --upgrade google-cloud-bigquery[bqstorage,pandas]\n",
        "!pip install --upgrade pandas-gbq"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: google-cloud-bigquery[bqstorage,pandas] in /usr/local/lib/python3.6/dist-packages (1.26.1)\n",
            "Requirement already satisfied, skipping upgrade: google-resumable-media<2.0dev,>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery[bqstorage,pandas]) (0.7.0)\n",
            "Requirement already satisfied, skipping upgrade: google-api-core<2.0dev,>=1.21.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery[bqstorage,pandas]) (1.22.0)\n",
            "Requirement already satisfied, skipping upgrade: six<2.0.0dev,>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery[bqstorage,pandas]) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: google-cloud-core<2.0dev,>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery[bqstorage,pandas]) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: pyarrow<2.0dev,>=0.16.0; extra == \"bqstorage\" in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery[bqstorage,pandas]) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: google-cloud-bigquery-storage<2.0.0dev,>=1.0.0; extra == \"bqstorage\" in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery[bqstorage,pandas]) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: grpcio<2.0dev,>=1.8.2; extra == \"bqstorage\" in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery[bqstorage,pandas]) (1.30.0)\n",
            "Requirement already satisfied, skipping upgrade: pandas>=0.17.1; extra == \"pandas\" in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery[bqstorage,pandas]) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: google-crc32c>=0.1.0; python_version >= \"3.5\" in /usr/local/lib/python3.6/dist-packages (from google-resumable-media<2.0dev,>=0.5.0->google-cloud-bigquery[bqstorage,pandas]) (0.1.0)\n",
            "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0dev,>=1.21.0->google-cloud-bigquery[bqstorage,pandas]) (1.52.0)\n",
            "Requirement already satisfied, skipping upgrade: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0dev,>=1.21.0->google-cloud-bigquery[bqstorage,pandas]) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.12.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0dev,>=1.21.0->google-cloud-bigquery[bqstorage,pandas]) (3.12.2)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0dev,>=1.21.0->google-cloud-bigquery[bqstorage,pandas]) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2.0dev,>=1.19.1 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0dev,>=1.21.0->google-cloud-bigquery[bqstorage,pandas]) (1.20.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=34.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0dev,>=1.21.0->google-cloud-bigquery[bqstorage,pandas]) (49.2.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.14 in /usr/local/lib/python3.6/dist-packages (from pyarrow<2.0dev,>=0.16.0; extra == \"bqstorage\"->google-cloud-bigquery[bqstorage,pandas]) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.1; extra == \"pandas\"->google-cloud-bigquery[bqstorage,pandas]) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: cffi>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from google-crc32c>=0.1.0; python_version >= \"3.5\"->google-resumable-media<2.0dev,>=0.5.0->google-cloud-bigquery[bqstorage,pandas]) (1.14.1)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0dev,>=1.21.0->google-cloud-bigquery[bqstorage,pandas]) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0dev,>=1.21.0->google-cloud-bigquery[bqstorage,pandas]) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0dev,>=1.21.0->google-cloud-bigquery[bqstorage,pandas]) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<2.0dev,>=1.21.0->google-cloud-bigquery[bqstorage,pandas]) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=1.19.1->google-api-core<2.0dev,>=1.21.0->google-cloud-bigquery[bqstorage,pandas]) (4.1.1)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.5\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=1.19.1->google-api-core<2.0dev,>=1.21.0->google-cloud-bigquery[bqstorage,pandas]) (4.6)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2.0dev,>=1.19.1->google-api-core<2.0dev,>=1.21.0->google-cloud-bigquery[bqstorage,pandas]) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0.0->google-crc32c>=0.1.0; python_version >= \"3.5\"->google-resumable-media<2.0dev,>=0.5.0->google-cloud-bigquery[bqstorage,pandas]) (2.20)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth<2.0dev,>=1.19.1->google-api-core<2.0dev,>=1.21.0->google-cloud-bigquery[bqstorage,pandas]) (0.4.8)\n",
            "Requirement already up-to-date: pandas-gbq in /usr/local/lib/python3.6/dist-packages (0.13.2)\n",
            "Requirement already satisfied, skipping upgrade: pydata-google-auth in /usr/local/lib/python3.6/dist-packages (from pandas-gbq) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib in /usr/local/lib/python3.6/dist-packages (from pandas-gbq) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from pandas-gbq) (49.2.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth in /usr/local/lib/python3.6/dist-packages (from pandas-gbq) (1.20.0)\n",
            "Requirement already satisfied, skipping upgrade: google-cloud-bigquery>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from pandas-gbq) (1.26.1)\n",
            "Requirement already satisfied, skipping upgrade: pandas>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from pandas-gbq) (1.0.5)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib->pandas-gbq) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3.5\" in /usr/local/lib/python3.6/dist-packages (from google-auth->pandas-gbq) (4.6)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth->pandas-gbq) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth->pandas-gbq) (4.1.1)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth->pandas-gbq) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: google-resumable-media<2.0dev,>=0.5.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery>=1.11.1->pandas-gbq) (0.7.0)\n",
            "Requirement already satisfied, skipping upgrade: google-cloud-core<2.0dev,>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery>=1.11.1->pandas-gbq) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: google-api-core<2.0dev,>=1.21.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-bigquery>=1.11.1->pandas-gbq) (1.22.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19.0->pandas-gbq) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19.0->pandas-gbq) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19.0->pandas-gbq) (2018.9)\n",
            "Requirement already satisfied, skipping upgrade: requests>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->pandas-gbq) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib->pandas-gbq) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth->pandas-gbq) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: google-crc32c>=0.1.0; python_version >= \"3.5\" in /usr/local/lib/python3.6/dist-packages (from google-resumable-media<2.0dev,>=0.5.0->google-cloud-bigquery>=1.11.1->pandas-gbq) (0.1.0)\n",
            "Requirement already satisfied, skipping upgrade: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0dev,>=1.21.0->google-cloud-bigquery>=1.11.1->pandas-gbq) (1.52.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.12.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0dev,>=1.21.0->google-cloud-bigquery>=1.11.1->pandas-gbq) (3.12.2)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib->pandas-gbq) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib->pandas-gbq) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib->pandas-gbq) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0.0->requests-oauthlib>=0.7.0->google-auth-oauthlib->pandas-gbq) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: cffi>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from google-crc32c>=0.1.0; python_version >= \"3.5\"->google-resumable-media<2.0dev,>=0.5.0->google-cloud-bigquery>=1.11.1->pandas-gbq) (1.14.1)\n",
            "Requirement already satisfied, skipping upgrade: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0.0->google-crc32c>=0.1.0; python_version >= \"3.5\"->google-resumable-media<2.0dev,>=0.5.0->google-cloud-bigquery>=1.11.1->pandas-gbq) (2.20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6y1_cKZGJ1C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "876c4ff0-1e60-41d6-c8fd-9c1dc16dedc0"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Authenticated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cPJ-kLQGQ0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bigquery --project machine-learning-msc df --use_bqstorage_api\n",
        "SELECT \n",
        "  COUNT(*) as total_rows\n",
        "FROM `machine-learning-msc.low_carbon_london.household_consumption_daily_agg` "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ts6qMG3PGlUL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "80b4ef7f-ee7a-44e0-a46b-fa3ea6d35b0e"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>total_rows</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14841792</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   total_rows\n",
              "0    14841792"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4sbx8kLH89t",
        "colab_type": "text"
      },
      "source": [
        "### **Importing Libraries‚è¨**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmVtQy38Ozr6",
        "colab_type": "text"
      },
      "source": [
        "##### Standard Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0JiDvCG3U4F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "outputId": "673857d1-5878-4432-cff3-ae12e57986d9"
      },
      "source": [
        "!pip install fbprophet\n",
        "!pip install MiniSom"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fbprophet in /usr/local/lib/python3.6/dist-packages (0.6)\n",
            "Requirement already satisfied: Cython>=0.22 in /usr/local/lib/python3.6/dist-packages (from fbprophet) (0.29.21)\n",
            "Requirement already satisfied: cmdstanpy==0.4 in /usr/local/lib/python3.6/dist-packages (from fbprophet) (0.4.0)\n",
            "Requirement already satisfied: pystan>=2.14 in /usr/local/lib/python3.6/dist-packages (from fbprophet) (2.19.1.1)\n",
            "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from fbprophet) (1.18.5)\n",
            "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.6/dist-packages (from fbprophet) (1.0.5)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from fbprophet) (3.2.2)\n",
            "Requirement already satisfied: LunarCalendar>=0.0.9 in /usr/local/lib/python3.6/dist-packages (from fbprophet) (0.0.9)\n",
            "Requirement already satisfied: convertdate>=2.1.2 in /usr/local/lib/python3.6/dist-packages (from fbprophet) (2.2.1)\n",
            "Requirement already satisfied: holidays>=0.9.5 in /usr/local/lib/python3.6/dist-packages (from fbprophet) (0.9.12)\n",
            "Requirement already satisfied: setuptools-git>=1.2 in /usr/local/lib/python3.6/dist-packages (from fbprophet) (1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.6/dist-packages (from fbprophet) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.4->fbprophet) (2018.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->fbprophet) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->fbprophet) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->fbprophet) (2.4.7)\n",
            "Requirement already satisfied: ephem>=3.7.5.3 in /usr/local/lib/python3.6/dist-packages (from LunarCalendar>=0.0.9->fbprophet) (3.7.7.1)\n",
            "Requirement already satisfied: pymeeus<=1,>=0.3.6 in /usr/local/lib/python3.6/dist-packages (from convertdate>=2.1.2->fbprophet) (0.3.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from holidays>=0.9.5->fbprophet) (1.15.0)\n",
            "Collecting MiniSom\n",
            "  Downloading https://files.pythonhosted.org/packages/9d/10/a1c1621000d5ca00c41695689551c1a4d6d245d7bbf099d81e067da3e8f2/MiniSom-2.2.6.tar.gz\n",
            "Building wheels for collected packages: MiniSom\n",
            "  Building wheel for MiniSom (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for MiniSom: filename=MiniSom-2.2.6-cp36-none-any.whl size=8525 sha256=07ddc633d161f778b86776ad0805ba81b81db372ff108602ec5fddb920b09639\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/c6/01/330066e36e1f7c826c96f656f9185822cfcdef0591315949ea\n",
            "Successfully built MiniSom\n",
            "Installing collected packages: MiniSom\n",
            "Successfully installed MiniSom-2.2.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr9vUfxAICRD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "a8c6558c-f1a1-4b53-d033-e5cf87855620"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt \n",
        "import random\n",
        "import datetime as dt\n",
        "\n",
        "from minisom import MiniSom\n",
        "from tqdm import tqdm\n",
        "from datetime import date\n",
        "from matplotlib.gridspec import GridSpec\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        " \n",
        "sns.set()\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQE0xrL4JT4F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas_gbq\n",
        "def output_to_bq(forecast, table_id, project_id='machine-learning-msc'):\n",
        "    pandas_gbq.to_gbq(forecast, table_id, project_id=project_id, if_exists='append')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq2GQCXqO2ZD",
        "colab_type": "text"
      },
      "source": [
        "##### Import Github Repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw_bhwfTY7U1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1e10b579-cc32-4f8b-e22e-a8711f14a144"
      },
      "source": [
        "%cd /content\n",
        "!ls"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "adc.json  mscproj  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtzIWQvBPESj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "3a0c8951-1478-488a-e6ff-1dbd4fcde1cf"
      },
      "source": [
        "!rm -rf mscproj\n",
        "!git clone https://github.com/shengy90/MSc-Project mscproj\n",
        "!git pull\n",
        "%cd /content/mscproj/\n",
        "!ls"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'mscproj'...\n",
            "remote: Enumerating objects: 26, done.\u001b[K\n",
            "remote: Counting objects: 100% (26/26), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 387 (delta 11), reused 13 (delta 6), pack-reused 361\u001b[K\n",
            "Receiving objects: 100% (387/387), 10.95 MiB | 4.58 MiB/s, done.\n",
            "Resolving deltas: 100% (209/209), done.\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "/content/mscproj\n",
            "bin\t     __init__.py  notebooks  requirements.txt  sql\n",
            "definitions  Makefile\t  README.md  run.py\t       src\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fnq6lOTUXbat",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%reload_ext autoreload \n",
        "%autoreload 2 \n",
        "from src.train_prophet import TrainProphet\n",
        "from src.train_clusters import TrainClusters\n",
        "from src.train_clusters import Normaliser"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uICiUW0YyfAZ",
        "colab_type": "text"
      },
      "source": [
        "# 2Ô∏è‚É£ **Training Up to 16 Agglomerative Clusters**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Us6xVpclRb_A",
        "colab_type": "text"
      },
      "source": [
        "### **Downloading Data from BQ**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VfmGhaSz4s4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bigquery --project machine-learning-msc df_test --use_bqstorage_api\n",
        "WITH stg1 AS (\n",
        "SELECT \n",
        "lcl_id,\n",
        "IF(acorn_grouped = \"Adversity\", 1, 0) AS adversity,\n",
        "IF(acorn_grouped = \"Affluent\", 1, 0) AS affluent,\n",
        "IF(acorn_grouped = \"Comfortable\", 1, 0) AS comfortable,\n",
        "FORMAT_DATETIME(\"%B\", DATETIME(ts)) AS month_name,\n",
        "dayofweek,\n",
        "hhourly_rank,\n",
        "ROUND(AVG(kwhh),4) AS hh_avg,\n",
        "ROUND(MAX(kwhh),4) AS hh_max,\n",
        "ROUND(MIN(kwhh),4) AS hh_min,\n",
        "ROUND(STDDEV(kwhh),4) AS hh_stddev\n",
        "\n",
        "FROM `machine-learning-msc.forecasting_20200719.test_set`\n",
        "WHERE train_test_split = 'test'\n",
        "AND ts >= '2012-11-01' AND ts < '2013-03-01'\n",
        "\n",
        "GROUP BY 1,2,3,4,5,6,7\n",
        ")\n",
        "\n",
        "SELECT \n",
        "*,\n",
        "ROW_NUMBER() OVER (PARTITION BY lcl_id, month_name ORDER BY dayofweek ASC, hhourly_rank ASC) AS weekly_rank\n",
        "FROM stg1 \n",
        "ORDER BY lcl_id, month_name, weekly_rank, hhourly_rank"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAKqRE4AIKRG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bigquery --project machine-learning-msc df_train --use_bqstorage_api\n",
        "WITH stg1 AS (\n",
        "SELECT \n",
        "lcl_id,\n",
        "IF(acorn_grouped = \"Adversity\", 1, 0) AS adversity,\n",
        "IF(acorn_grouped = \"Affluent\", 1, 0) AS affluent,\n",
        "IF(acorn_grouped = \"Comfortable\", 1, 0) AS comfortable,\n",
        "FORMAT_DATETIME(\"%B\", DATETIME(ts)) AS month_name,\n",
        "dayofweek,\n",
        "hhourly_rank,\n",
        "ROUND(AVG(kwhh),4) AS hh_avg,\n",
        "ROUND(MAX(kwhh),4) AS hh_max,\n",
        "ROUND(MIN(kwhh),4) AS hh_min,\n",
        "ROUND(STDDEV(kwhh),4) AS hh_stddev\n",
        "\n",
        "FROM `machine-learning-msc.forecasting_20200719.train_set`\n",
        "WHERE train_test_split = 'train'\n",
        "AND ts >= '2012-11-01' AND ts < '2013-03-01'\n",
        "\n",
        "GROUP BY 1,2,3,4,5,6,7\n",
        ")\n",
        "\n",
        "SELECT \n",
        "*,\n",
        "ROW_NUMBER() OVER (PARTITION BY lcl_id, month_name ORDER BY dayofweek ASC, hhourly_rank ASC) AS weekly_rank\n",
        "FROM stg1 \n",
        "ORDER BY lcl_id, month_name, weekly_rank, hhourly_rank"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ly1lLTEs0yxu",
        "colab_type": "text"
      },
      "source": [
        "### **Normalise Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNhVyPMcyjAG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "value_list = ['hh_avg']\n",
        "column_list = ['month_name', 'weekly_rank']\n",
        "normaliser = Normaliser(value_list, column_list)\n",
        "norm_df_train = normaliser.fit(df_train)\n",
        "norm_df_test = normaliser.predict(df_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwTqc9CP_DWF",
        "colab_type": "text"
      },
      "source": [
        "### **1. Training Up to 16 Agglomerative Clusters**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTR9D7sm_F6X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "5e3b4b84-fd16-42a9-89d1-14e2ff07dd27"
      },
      "source": [
        "for i in range(16):\n",
        "    cluster_num = i+1\n",
        "    print(f\"Training {cluster_num} clusters....\")\n",
        "    agglo_cluster = TrainClusters(cluster_type=\"agglo\")\n",
        "    agglo_cluster.fit(norm_df_train, cluster_num=cluster_num)\n",
        "\n",
        "    train_pred = agglo_cluster.predict(norm_df_train)\n",
        "    test_pred = agglo_cluster.predict(norm_df_test)\n",
        "\n",
        "    train_pred['train_test_split'] = \"train\"\n",
        "    test_pred['train_test_split'] = \"test\"\n",
        "    \n",
        "    agglo_results = pd.concat([train_pred[['lcl_id','cluster','train_test_split']], test_pred[['lcl_id','cluster','train_test_split']]])\n",
        "    agglo_results['cluster'] = agglo_results['cluster'].astype(float)\n",
        "    agglo_results['num_clusters'] = cluster_num\n",
        "    agglo_results['cluster_type'] = 'agglo'\n",
        "\n",
        "    output_to_bq(agglo_results, 'clusters_20200802.agglo_16_clusters')\n",
        "    print(\"Upload to BQ completed! üéâ\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training 1 clusters....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1it [00:03,  3.17s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Upload to BQ completed! üéâ\n",
            "Training 2 clusters....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1it [00:03,  3.61s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Upload to BQ completed! üéâ\n",
            "Training 3 clusters....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1it [00:02,  2.77s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Upload to BQ completed! üéâ\n",
            "Training 4 clusters....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1it [00:05,  5.54s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Upload to BQ completed! üéâ\n",
            "Training 5 clusters....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1it [00:02,  2.73s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Upload to BQ completed! üéâ\n",
            "Training 6 clusters....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1it [00:02,  2.79s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Upload to BQ completed! üéâ\n",
            "Training 7 clusters....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1it [00:04,  4.18s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Upload to BQ completed! üéâ\n",
            "Training 8 clusters....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1it [00:02,  2.13s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Upload to BQ completed! üéâ\n",
            "Training 9 clusters....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1it [00:02,  2.77s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Upload to BQ completed! üéâ\n",
            "Training 10 clusters....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1it [00:03,  3.97s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Upload to BQ completed! üéâ\n",
            "Training 11 clusters....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1it [00:04,  4.24s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Upload to BQ completed! üéâ\n",
            "Training 12 clusters....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1it [00:05,  5.18s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Upload to BQ completed! üéâ\n",
            "Training 13 clusters....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1it [00:09,  9.65s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Upload to BQ completed! üéâ\n",
            "Training 14 clusters....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1it [00:04,  4.29s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Upload to BQ completed! üéâ\n",
            "Training 15 clusters....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1it [00:04,  4.08s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Upload to BQ completed! üéâ\n",
            "Training 16 clusters....\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1it [00:03,  3.71s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Upload to BQ completed! üéâ\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQMQ6kACRtgv",
        "colab_type": "text"
      },
      "source": [
        "### **2. Evaluating Clusters**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0hNFBIouR0tx",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "##### **Downloading Data from BQ**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iI2320mRx7_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bigquery --project machine-learning-msc df_train --use_bqstorage_api\n",
        "SELECT \n",
        "train.lcl_id,\n",
        "train.ts AS ds,\n",
        "train.kwhh AS y,\n",
        "weather.air_temperature\n",
        "\n",
        "FROM `machine-learning-msc.forecasting_20200719.train_set` train \n",
        "LEFT JOIN `machine-learning-msc.london_heathrow_hourly_weather_data.london_heathrow_hourly_weather` weather \n",
        "  ON TIMESTAMP_TRUNC(weather.ts, HOUR) = TIMESTAMP_TRUNC(train.ts, hour)\n",
        "\n",
        "WHERE train.ts >= '2012-11-01' AND train.ts < '2013-03-01'\n",
        "ORDER BY 1,2 ASC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ph4St_mDUcKc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bigquery --project machine-learning-msc df_test --use_bqstorage_api\n",
        "SELECT \n",
        "train.lcl_id,\n",
        "train.ts AS ds,\n",
        "train.kwhh AS y,\n",
        "weather.air_temperature\n",
        "\n",
        "FROM `machine-learning-msc.forecasting_20200719.test_set` train \n",
        "LEFT JOIN `machine-learning-msc.london_heathrow_hourly_weather_data.london_heathrow_hourly_weather` weather \n",
        "  ON TIMESTAMP_TRUNC(weather.ts, HOUR) = TIMESTAMP_TRUNC(train.ts, hour)\n",
        "  \n",
        "\n",
        "WHERE train.ts >= '2012-11-01' AND train.ts < '2013-03-01'\n",
        "ORDER BY 1,2 ASC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZQgpcwMUk2b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "56325b55-6ed0-43e2-ba12-d6b31fcc89ff"
      },
      "source": [
        "df_train['ds'] = df_train['ds'].dt.tz_localize(None) # remove timezones \n",
        "df_test['ds'] = df_test['ds'].dt.tz_localize(None) # remove timezones \n",
        "\n",
        "print(df_train.shape, df_test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(15439123, 4) (5758620, 4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7DdLHFDUmVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bigquery --project machine-learning-msc clusters --use_bqstorage_api\n",
        "SELECT * FROM `machine-learning-msc.clusters_20200802.agglo_16_clusters`"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7kHACW-Vhzw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "47625caa-bb53-406e-ab93-f944f4b2c10e"
      },
      "source": [
        "clusters.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lcl_id</th>\n",
              "      <th>cluster</th>\n",
              "      <th>train_test_split</th>\n",
              "      <th>num_clusters</th>\n",
              "      <th>cluster_type</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>MAC000024</td>\n",
              "      <td>0.0</td>\n",
              "      <td>test</td>\n",
              "      <td>3</td>\n",
              "      <td>agglo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>MAC000034</td>\n",
              "      <td>0.0</td>\n",
              "      <td>test</td>\n",
              "      <td>3</td>\n",
              "      <td>agglo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>MAC000312</td>\n",
              "      <td>0.0</td>\n",
              "      <td>test</td>\n",
              "      <td>3</td>\n",
              "      <td>agglo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>MAC000415</td>\n",
              "      <td>0.0</td>\n",
              "      <td>test</td>\n",
              "      <td>3</td>\n",
              "      <td>agglo</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>MAC000557</td>\n",
              "      <td>0.0</td>\n",
              "      <td>test</td>\n",
              "      <td>3</td>\n",
              "      <td>agglo</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      lcl_id  cluster train_test_split  num_clusters cluster_type\n",
              "0  MAC000024      0.0             test             3        agglo\n",
              "1  MAC000034      0.0             test             3        agglo\n",
              "2  MAC000312      0.0             test             3        agglo\n",
              "3  MAC000415      0.0             test             3        agglo\n",
              "4  MAC000557      0.0             test             3        agglo"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qJFF57TVnTJ",
        "colab_type": "text"
      },
      "source": [
        "##### **Utility Functions..**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbhxIK8nVlTh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_clusters(df_train, df_test, test_period=\"2013-02-01\"):\n",
        "    forecast_dict = {}\n",
        "    test_global_fc = pd.DataFrame()\n",
        "    train_global_fc = pd.DataFrame()\n",
        "    clusters = df_train.groupby('cluster').count().index.to_list()\n",
        "    for cluster in clusters:\n",
        "        cluster_dict = {} \n",
        "        print(f\"\\nTraining cluster: {cluster}\") \n",
        "        print(\"---------------------------\")\n",
        "        df_train_cluster = df_train.query(f\"cluster=={cluster}\").copy()\n",
        "        df_test_cluster = df_test.query(f\"cluster=={cluster}\").copy()\n",
        "        model = TrainProphet(test_period)\n",
        "        model.fit(df_train_cluster)\n",
        "        model.evaluate_test_global_mape(df_test_cluster)\n",
        "        cluster_dict['model'] = model \n",
        "        forecast_dict[f'cluster_{cluster}']=cluster_dict\n",
        "        test_global_fc = pd.concat([test_global_fc, model.test_forecast])\n",
        "\n",
        "        train_forecast = df_train[['cluster','ds','y']].copy()\n",
        "        train_forecast['max_households'] = df_train['households_num'].max()\n",
        "        train_forecast = train_forecast.merge(model.forecast[['ds', 'yhat']], left_on='ds', right_on='ds')\n",
        "        train_forecast['y_global'] = train_forecast['y'] * train_forecast['max_households']\n",
        "        train_forecast['yhat_global'] = train_forecast['yhat'] * train_forecast['max_households']\n",
        "        train_global_fc = pd.concat([train_global_fc, train_forecast])\n",
        "\n",
        "    return forecast_dict, test_global_fc, train_global_fc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QEr7g172X0zN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_timeseries(df, clusters_df, cluster_type, num_clusters):\n",
        "    clusters = clusters_df.query(f\"cluster_type=='{cluster_type}' and num_clusters=={num_clusters}\")\n",
        "    out_df = df.merge(clusters[['lcl_id','cluster']], left_on='lcl_id', right_on='lcl_id')\n",
        "    households_num = pd.DataFrame(out_df.groupby('cluster')['lcl_id'].nunique())\n",
        "    households_num.rename(columns={'lcl_id':'households_num'}, inplace=True)\n",
        "\n",
        "    timeseries = out_df.groupby(['cluster','ds']).mean().reset_index()\n",
        "    timeseries = timeseries.merge(households_num, left_on='cluster', right_on='cluster')\n",
        "    return timeseries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zV6MR_peDBX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_cluster_forecast(df_train, df_test, cluster_type, clusters, cluster_list):\n",
        "    results_dict = {}\n",
        "    \n",
        "    for cluster in cluster_list:\n",
        "        cluster_forecast_dict = {}\n",
        "        print(f\"\\n ----------------------------------\")\n",
        "        print(f\"|Total number of clusters: {cluster}...   |\")\n",
        "        print(f\" ----------------------------------\")\n",
        "\n",
        "        train = get_timeseries(df_train, clusters, cluster_type=cluster_type, num_clusters=cluster)\n",
        "        test = get_timeseries(df_test, clusters, cluster_type=cluster_type, num_clusters=cluster)\n",
        "\n",
        "        model_dict, global_test, global_train = train_clusters(train, test)\n",
        "\n",
        "        cluster_forecast_dict['model'] = model_dict\n",
        "        cluster_forecast_dict['global_test'] = global_test\n",
        "        cluster_forecast_dict['global_train'] = global_train \n",
        "\n",
        "        results_dict[f\"num_clusters_{cluster}\"] = cluster_forecast_dict \n",
        "\n",
        "    return results_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kky74E1-V67I",
        "colab_type": "text"
      },
      "source": [
        "#####**Training Forecasts**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suydcguUftV0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "397cdbf2-01e6-4209-97f8-63e086818caa"
      },
      "source": [
        "cluster_list = [2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]\n",
        "agglo_results = train_cluster_forecast(df_train, df_test, 'agglo', clusters, cluster_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " ----------------------------------\n",
            "|Total number of clusters: 2...   |\n",
            " ----------------------------------\n",
            "\n",
            "Training cluster: 0.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 7.790572916666669\n",
            "Test Mean Absolute Percentage Error: 8.23\n",
            "\n",
            "Training cluster: 1.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 8.77916666666666\n",
            "Test Mean Absolute Percentage Error: 29.25\n",
            "\n",
            " ----------------------------------\n",
            "|Total number of clusters: 3...   |\n",
            " ----------------------------------\n",
            "\n",
            "Training cluster: 0.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 8.569873511904763\n",
            "Test Mean Absolute Percentage Error: 64.62\n",
            "\n",
            "Training cluster: 1.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 8.77916666666666\n",
            "Test Mean Absolute Percentage Error: 156.81\n",
            "\n",
            "Training cluster: 2.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 7.447537202380951\n",
            "Test Mean Absolute Percentage Error: 8.110000000000001\n",
            "\n",
            " ----------------------------------\n",
            "|Total number of clusters: 4...   |\n",
            " ----------------------------------\n",
            "\n",
            "Training cluster: 0.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 8.77916666666666\n",
            "Test Mean Absolute Percentage Error: 156.81\n",
            "\n",
            "Training cluster: 1.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 8.647232142857149\n",
            "Test Mean Absolute Percentage Error: 52.38\n",
            "\n",
            "Training cluster: 2.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 7.447537202380951\n",
            "Test Mean Absolute Percentage Error: 8.110000000000001\n",
            "\n",
            "Training cluster: 3.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 79.13171875000015\n",
            "Test Mean Absolute Percentage Error: 72.17\n",
            "\n",
            " ----------------------------------\n",
            "|Total number of clusters: 5...   |\n",
            " ----------------------------------\n",
            "\n",
            "Training cluster: 0.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 8.647232142857149\n",
            "Test Mean Absolute Percentage Error: 52.38\n",
            "\n",
            "Training cluster: 1.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 14.201674107142859\n",
            "Test Mean Absolute Percentage Error: 27.939999999999998\n",
            "\n",
            "Training cluster: 2.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 7.447537202380951\n",
            "Test Mean Absolute Percentage Error: 8.110000000000001\n",
            "\n",
            "Training cluster: 3.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 79.13171875000015\n",
            "Test Mean Absolute Percentage Error: 239.78000000000003\n",
            "\n",
            "Training cluster: 4.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 9.132202380952364\n",
            "Test Mean Absolute Percentage Error: 27.560000000000002\n",
            "\n",
            " ----------------------------------\n",
            "|Total number of clusters: 6...   |\n",
            " ----------------------------------\n",
            "\n",
            "Training cluster: 0.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 14.201674107142859\n",
            "Test Mean Absolute Percentage Error: 27.939999999999998\n",
            "\n",
            "Training cluster: 1.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 9.132202380952364\n",
            "Test Mean Absolute Percentage Error: 163.72\n",
            "\n",
            "Training cluster: 2.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 7.447537202380951\n",
            "Test Mean Absolute Percentage Error: 8.110000000000001\n",
            "\n",
            "Training cluster: 3.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 79.13171875000015\n",
            "Test Mean Absolute Percentage Error: 57.79\n",
            "\n",
            "Training cluster: 4.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 9.232142857142842\n",
            "Test Mean Absolute Percentage Error: 26.479999999999997\n",
            "\n",
            "Training cluster: 5.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 8.893110119047597\n",
            "Test Mean Absolute Percentage Error: 68.35\n",
            "\n",
            " ----------------------------------\n",
            "|Total number of clusters: 7...   |\n",
            " ----------------------------------\n",
            "\n",
            "Training cluster: 0.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 9.132202380952364\n",
            "Test Mean Absolute Percentage Error: 163.72\n",
            "\n",
            "Training cluster: 1.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 79.13171875000015\n",
            "Test Mean Absolute Percentage Error: 57.79\n",
            "\n",
            "Training cluster: 2.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 7.447537202380951\n",
            "Test Mean Absolute Percentage Error: 8.110000000000001\n",
            "\n",
            "Training cluster: 3.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 15.594903273809537\n",
            "Test Mean Absolute Percentage Error: 53.22\n",
            "\n",
            "Training cluster: 4.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 9.232142857142842\n",
            "Test Mean Absolute Percentage Error: 26.479999999999997\n",
            "\n",
            "Training cluster: 5.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 8.893110119047597\n",
            "Test Mean Absolute Percentage Error: 68.35\n",
            "\n",
            "Training cluster: 6.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 1354133157166.448\n",
            "Test Mean Absolute Percentage Error: 100.8\n",
            "\n",
            " ----------------------------------\n",
            "|Total number of clusters: 8...   |\n",
            " ----------------------------------\n",
            "\n",
            "Training cluster: 0.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 79.13171875000015\n",
            "Test Mean Absolute Percentage Error: 57.79\n",
            "\n",
            "Training cluster: 1.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 11.302418154761911\n",
            "Test Mean Absolute Percentage Error: 60.6\n",
            "\n",
            "Training cluster: 2.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 7.447537202380951\n",
            "Test Mean Absolute Percentage Error: 8.110000000000001\n",
            "\n",
            "Training cluster: 3.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 15.594903273809537\n",
            "Test Mean Absolute Percentage Error: 53.22\n",
            "\n",
            "Training cluster: 4.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 9.232142857142842\n",
            "Test Mean Absolute Percentage Error: 71.32\n",
            "\n",
            "Training cluster: 5.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 8.893110119047597\n",
            "Test Mean Absolute Percentage Error: 68.35\n",
            "\n",
            "Training cluster: 6.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 1354133157166.448\n",
            "Test Mean Absolute Percentage Error: 100.8\n",
            "\n",
            "Training cluster: 7.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 9.861770833333335\n",
            "Test Mean Absolute Percentage Error: 77.81\n",
            "\n",
            " ----------------------------------\n",
            "|Total number of clusters: 9...   |\n",
            " ----------------------------------\n",
            "\n",
            "Training cluster: 0.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 7.447537202380951\n",
            "Test Mean Absolute Percentage Error: 8.110000000000001\n",
            "\n",
            "Training cluster: 1.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 11.302418154761911\n",
            "Test Mean Absolute Percentage Error: 60.6\n",
            "\n",
            "Training cluster: 2.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 87.33370535714285\n",
            "Test Mean Absolute Percentage Error: 59.24\n",
            "\n",
            "Training cluster: 3.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 15.594903273809537\n",
            "Test Mean Absolute Percentage Error: 53.22\n",
            "\n",
            "Training cluster: 4.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 9.232142857142842\n",
            "Test Mean Absolute Percentage Error: 71.32\n",
            "\n",
            "Training cluster: 5.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 8.893110119047597\n",
            "Test Mean Absolute Percentage Error: 68.35\n",
            "\n",
            "Training cluster: 6.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 1354133157166.448\n",
            "Test Mean Absolute Percentage Error: 100.8\n",
            "\n",
            "Training cluster: 7.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 9.861770833333335\n",
            "Test Mean Absolute Percentage Error: 77.81\n",
            "\n",
            "Training cluster: 8.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 77.82642857142854\n",
            "Test Mean Absolute Percentage Error: 209.22\n",
            "\n",
            " ----------------------------------\n",
            "|Total number of clusters: 10...   |\n",
            " ----------------------------------\n",
            "\n",
            "Training cluster: 0.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 11.302418154761911\n",
            "Test Mean Absolute Percentage Error: 60.6\n",
            "\n",
            "Training cluster: 1.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 9.232142857142842\n",
            "Test Mean Absolute Percentage Error: 63.22\n",
            "\n",
            "Training cluster: 2.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 87.33370535714285\n",
            "Test Mean Absolute Percentage Error: 59.24\n",
            "\n",
            "Training cluster: 3.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 15.594903273809537\n",
            "Test Mean Absolute Percentage Error: 457.43\n",
            "\n",
            "Training cluster: 4.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 6.3901339285714345\n",
            "Test Mean Absolute Percentage Error: 69.08\n",
            "\n",
            "Training cluster: 5.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 8.893110119047597\n",
            "Test Mean Absolute Percentage Error: 68.35\n",
            "\n",
            "Training cluster: 6.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 1354133157166.448\n",
            "Test Mean Absolute Percentage Error: 100.8\n",
            "\n",
            "Training cluster: 7.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 9.861770833333335\n",
            "Test Mean Absolute Percentage Error: 321.53999999999996\n",
            "\n",
            "Training cluster: 8.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 77.82642857142854\n",
            "Test Mean Absolute Percentage Error: 209.22\n",
            "\n",
            "Training cluster: 9.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 8.706964285714278\n",
            "Test Mean Absolute Percentage Error: 117.61999999999999\n",
            "\n",
            " ----------------------------------\n",
            "|Total number of clusters: 11...   |\n",
            " ----------------------------------\n",
            "\n",
            "Training cluster: 0.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 9.232142857142842\n",
            "Test Mean Absolute Percentage Error: 26.06\n",
            "\n",
            "Training cluster: 1.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 15.594903273809537\n",
            "Test Mean Absolute Percentage Error: 53.22\n",
            "\n",
            "Training cluster: 2.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 87.33370535714285\n",
            "Test Mean Absolute Percentage Error: 59.24\n",
            "\n",
            "Training cluster: 3.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 26.53484375000003\n",
            "Test Mean Absolute Percentage Error: 226.21000000000004\n",
            "\n",
            "Training cluster: 4.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 11.935892857142884\n",
            "Test Mean Absolute Percentage Error: 281.40999999999997\n",
            "\n",
            "Training cluster: 5.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 8.893110119047597\n",
            "Test Mean Absolute Percentage Error: 68.35\n",
            "\n",
            "Training cluster: 6.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 1354133157166.448\n",
            "Test Mean Absolute Percentage Error: 100.8\n",
            "\n",
            "Training cluster: 7.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 9.861770833333335\n",
            "Test Mean Absolute Percentage Error: 321.53999999999996\n",
            "\n",
            "Training cluster: 8.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 77.82642857142854\n",
            "Test Mean Absolute Percentage Error: 209.22\n",
            "\n",
            "Training cluster: 9.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 8.706964285714278\n",
            "Test Mean Absolute Percentage Error: 117.61999999999999\n",
            "\n",
            "Training cluster: 10.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 6.3901339285714345\n",
            "Test Mean Absolute Percentage Error: inf\n",
            "\n",
            " ----------------------------------\n",
            "|Total number of clusters: 12...   |\n",
            " ----------------------------------\n",
            "\n",
            "Training cluster: 0.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 87.33370535714285\n",
            "Test Mean Absolute Percentage Error: 74.31\n",
            "\n",
            "Training cluster: 1.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 15.594903273809537\n",
            "Test Mean Absolute Percentage Error: 637.4\n",
            "\n",
            "Training cluster: 2.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 8.893110119047597\n",
            "Test Mean Absolute Percentage Error: 47.99\n",
            "\n",
            "Training cluster: 3.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 26.53484375000003\n",
            "Test Mean Absolute Percentage Error: 226.21000000000004\n",
            "\n",
            "Training cluster: 4.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 11.935892857142884\n",
            "Test Mean Absolute Percentage Error: 80.65\n",
            "\n",
            "Training cluster: 5.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 11.30706845238095\n",
            "Test Mean Absolute Percentage Error: 147.13\n",
            "\n",
            "Training cluster: 6.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 1354133157166.448\n",
            "Test Mean Absolute Percentage Error: 100.8\n",
            "\n",
            "Training cluster: 7.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 9.861770833333335\n",
            "Test Mean Absolute Percentage Error: 321.53999999999996\n",
            "\n",
            "Training cluster: 8.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 77.82642857142854\n",
            "Test Mean Absolute Percentage Error: 209.22\n",
            "\n",
            "Training cluster: 9.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 8.706964285714278\n",
            "Test Mean Absolute Percentage Error: 117.61999999999999\n",
            "\n",
            "Training cluster: 10.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 6.3901339285714345\n",
            "Test Mean Absolute Percentage Error: inf\n",
            "\n",
            "Training cluster: 11.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 10.729970238095243\n",
            "Test Mean Absolute Percentage Error: 32.519999999999996\n",
            "\n",
            " ----------------------------------\n",
            "|Total number of clusters: 13...   |\n",
            " ----------------------------------\n",
            "\n",
            "Training cluster: 0.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 8.893110119047597\n",
            "Test Mean Absolute Percentage Error: 47.99\n",
            "\n",
            "Training cluster: 1.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 15.594903273809537\n",
            "Test Mean Absolute Percentage Error: 637.4\n",
            "\n",
            "Training cluster: 2.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 11.30706845238095\n",
            "Test Mean Absolute Percentage Error: 66.44\n",
            "\n",
            "Training cluster: 3.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 26.53484375000003\n",
            "Test Mean Absolute Percentage Error: 226.21000000000004\n",
            "\n",
            "Training cluster: 4.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 11.935892857142884\n",
            "Test Mean Absolute Percentage Error: 80.65\n",
            "\n",
            "Training cluster: 5.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 60.097165178571416\n",
            "Test Mean Absolute Percentage Error: 62.83\n",
            "\n",
            "Training cluster: 6.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 1354133157166.448\n",
            "Test Mean Absolute Percentage Error: 100.8\n",
            "\n",
            "Training cluster: 7.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 9.861770833333335\n",
            "Test Mean Absolute Percentage Error: 321.53999999999996\n",
            "\n",
            "Training cluster: 8.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 77.82642857142854\n",
            "Test Mean Absolute Percentage Error: 209.22\n",
            "\n",
            "Training cluster: 9.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 8.706964285714278\n",
            "Test Mean Absolute Percentage Error: 117.61999999999999\n",
            "\n",
            "Training cluster: 10.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 6.3901339285714345\n",
            "Test Mean Absolute Percentage Error: inf\n",
            "\n",
            "Training cluster: 11.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 10.729970238095243\n",
            "Test Mean Absolute Percentage Error: 32.519999999999996\n",
            "\n",
            "Training cluster: 12.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 179.8139062499999\n",
            "Test Mean Absolute Percentage Error: 85.13\n",
            "\n",
            " ----------------------------------\n",
            "|Total number of clusters: 14...   |\n",
            " ----------------------------------\n",
            "\n",
            "Training cluster: 0.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 11.30706845238095\n",
            "Test Mean Absolute Percentage Error: 66.53999999999999\n",
            "\n",
            "Training cluster: 1.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 15.594903273809537\n",
            "Test Mean Absolute Percentage Error: 244.58\n",
            "\n",
            "Training cluster: 2.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 60.097165178571416\n",
            "Test Mean Absolute Percentage Error: 71.63000000000001\n",
            "\n",
            "Training cluster: 3.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 26.53484375000003\n",
            "Test Mean Absolute Percentage Error: 226.21000000000004\n",
            "\n",
            "Training cluster: 4.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 11.935892857142884\n",
            "Test Mean Absolute Percentage Error: 734.9300000000001\n",
            "\n",
            "Training cluster: 5.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 10.067194940476197\n",
            "Test Mean Absolute Percentage Error: 89.77000000000001\n",
            "\n",
            "Training cluster: 6.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 9.004270833333344\n",
            "Test Mean Absolute Percentage Error: 71.61\n",
            "\n",
            "Training cluster: 7.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 9.861770833333335\n",
            "Test Mean Absolute Percentage Error: 321.53999999999996\n",
            "\n",
            "Training cluster: 8.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 77.82642857142854\n",
            "Test Mean Absolute Percentage Error: 209.22\n",
            "\n",
            "Training cluster: 9.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 8.706964285714278\n",
            "Test Mean Absolute Percentage Error: 117.61999999999999\n",
            "\n",
            "Training cluster: 10.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 6.3901339285714345\n",
            "Test Mean Absolute Percentage Error: inf\n",
            "\n",
            "Training cluster: 11.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 10.729970238095243\n",
            "Test Mean Absolute Percentage Error: 32.519999999999996\n",
            "\n",
            "Training cluster: 12.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 179.8139062499999\n",
            "Test Mean Absolute Percentage Error: 85.13\n",
            "\n",
            "Training cluster: 13.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 1354133157166.448\n",
            "Test Mean Absolute Percentage Error: 100.8\n",
            "\n",
            " ----------------------------------\n",
            "|Total number of clusters: 15...   |\n",
            " ----------------------------------\n",
            "\n",
            "Training cluster: 0.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 60.097165178571416\n",
            "Test Mean Absolute Percentage Error: 71.63000000000001\n",
            "\n",
            "Training cluster: 1.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 15.594903273809537\n",
            "Test Mean Absolute Percentage Error: 244.58\n",
            "\n",
            "Training cluster: 2.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 12.842812500000015\n",
            "Test Mean Absolute Percentage Error: 191.88\n",
            "\n",
            "Training cluster: 3.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 26.53484375000003\n",
            "Test Mean Absolute Percentage Error: 226.21000000000004\n",
            "\n",
            "Training cluster: 4.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 11.935892857142884\n",
            "Test Mean Absolute Percentage Error: 734.9300000000001\n",
            "\n",
            "Training cluster: 5.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 10.067194940476197\n",
            "Test Mean Absolute Percentage Error: 50.57000000000001\n",
            "\n",
            "Training cluster: 6.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 9.004270833333344\n",
            "Test Mean Absolute Percentage Error: 71.61\n",
            "\n",
            "Training cluster: 7.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 9.861770833333335\n",
            "Test Mean Absolute Percentage Error: 321.53999999999996\n",
            "\n",
            "Training cluster: 8.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 77.82642857142854\n",
            "Test Mean Absolute Percentage Error: 209.22\n",
            "\n",
            "Training cluster: 9.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 8.706964285714278\n",
            "Test Mean Absolute Percentage Error: 117.61999999999999\n",
            "\n",
            "Training cluster: 10.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 6.3901339285714345\n",
            "Test Mean Absolute Percentage Error: inf\n",
            "\n",
            "Training cluster: 11.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 10.729970238095243\n",
            "Test Mean Absolute Percentage Error: 60.25\n",
            "\n",
            "Training cluster: 12.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 179.8139062499999\n",
            "Test Mean Absolute Percentage Error: 85.13\n",
            "\n",
            "Training cluster: 13.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 1354133157166.448\n",
            "Test Mean Absolute Percentage Error: 100.8\n",
            "\n",
            "Training cluster: 14.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 13.692544642857145\n",
            "Test Mean Absolute Percentage Error: 80.54\n",
            "\n",
            " ----------------------------------\n",
            "|Total number of clusters: 16...   |\n",
            " ----------------------------------\n",
            "\n",
            "Training cluster: 0.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 15.594903273809537\n",
            "Test Mean Absolute Percentage Error: 244.58\n",
            "\n",
            "Training cluster: 1.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 26.53484375000003\n",
            "Test Mean Absolute Percentage Error: 30.630000000000003\n",
            "\n",
            "Training cluster: 2.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 12.842812500000015\n",
            "Test Mean Absolute Percentage Error: 191.88\n",
            "\n",
            "Training cluster: 3.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 9.861770833333335\n",
            "Test Mean Absolute Percentage Error: 77.81\n",
            "\n",
            "Training cluster: 4.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 11.935892857142884\n",
            "Test Mean Absolute Percentage Error: 734.9300000000001\n",
            "\n",
            "Training cluster: 5.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 10.067194940476197\n",
            "Test Mean Absolute Percentage Error: 50.57000000000001\n",
            "\n",
            "Training cluster: 6.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 9.004270833333344\n",
            "Test Mean Absolute Percentage Error: 71.61\n",
            "\n",
            "Training cluster: 7.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 71.08560267857139\n",
            "Test Mean Absolute Percentage Error: 72.66\n",
            "\n",
            "Training cluster: 8.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 77.82642857142854\n",
            "Test Mean Absolute Percentage Error: 209.22\n",
            "\n",
            "Training cluster: 9.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 8.706964285714278\n",
            "Test Mean Absolute Percentage Error: 117.61999999999999\n",
            "\n",
            "Training cluster: 10.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 6.3901339285714345\n",
            "Test Mean Absolute Percentage Error: inf\n",
            "\n",
            "Training cluster: 11.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 10.729970238095243\n",
            "Test Mean Absolute Percentage Error: 60.25\n",
            "\n",
            "Training cluster: 12.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 179.8139062499999\n",
            "Test Mean Absolute Percentage Error: 85.13\n",
            "\n",
            "Training cluster: 13.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 1354133157166.448\n",
            "Test Mean Absolute Percentage Error: 100.8\n",
            "\n",
            "Training cluster: 14.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 13.692544642857145\n",
            "Test Mean Absolute Percentage Error: 80.54\n",
            "\n",
            "Training cluster: 15.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 80.3647767857143\n",
            "Test Mean Absolute Percentage Error: 244.83\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NSnnnoU4lqll",
        "colab_type": "text"
      },
      "source": [
        "##### **Evaluate Forecast**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bk1MHVS1l24b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ce01cd34-fe45-4491-f091-9e4c9a51165a"
      },
      "source": [
        "train_global_results = agglo_results['num_clusters_2']['global_train'].groupby('ds')[['y_global','yhat_global']].sum()\n",
        "test_global_results = agglo_results['num_clusters_2']['global_test'].groupby('ds')[['y_global','yhat_global']].sum()\n",
        "\n",
        "train_global_mape = np.round(np.mean(np.abs(train_global_results['yhat_global']/train_global_results['y_global']-1)),4)*100\n",
        "test_global_mape = np.round(np.mean(np.abs(test_global_results['yhat_global']/test_global_results['y_global']-1)),4)*100\n",
        "\n",
        "print(train_global_mape, test_global_mape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6.660000000000001 9.180000000000001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GonLw8Vnl2yL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_results(results_dict):\n",
        "    for num_clusters in results_dict.keys():\n",
        "        train_results = results_dict[num_clusters]['global_train'].groupby('ds')[['y_global','yhat_global']].sum()\n",
        "        test_results = results_dict[num_clusters]['global_test'].groupby('ds')[['y_global', 'yhat_global']].sum() \n",
        "\n",
        "        train_global_mape = np.round(np.mean(np.abs(train_results['yhat_global']/train_results['y_global']-1)),4)*100\n",
        "        test_global_mape = np.round(np.mean(np.abs(test_results['yhat_global']/test_results['y_global']-1)),4)*100\n",
        "        results_dict[num_clusters]['train_global_mape'] = train_global_mape\n",
        "        results_dict[num_clusters]['test_global_mape'] = test_global_mape\n",
        "\n",
        "        print(f\"Number of Clusters = {num_clusters}: Train Global MAPE: {train_global_mape}. Test Global MAPE: {test_global_mape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIAzzlGLl2nO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "574190cf-628e-4eda-97a4-f25d4588361b"
      },
      "source": [
        "evaluate_results(agglo_results)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of Clusters = num_clusters_2: Train Global MAPE: 6.660000000000001. Test Global MAPE: 9.180000000000001\n",
            "Number of Clusters = num_clusters_3: Train Global MAPE: 6.569999999999999. Test Global MAPE: 71.78\n",
            "Number of Clusters = num_clusters_4: Train Global MAPE: 12.22. Test Global MAPE: 74.11\n",
            "Number of Clusters = num_clusters_5: Train Global MAPE: 9.27. Test Global MAPE: 77.21000000000001\n",
            "Number of Clusters = num_clusters_6: Train Global MAPE: 8.64. Test Global MAPE: 58.13\n",
            "Number of Clusters = num_clusters_7: Train Global MAPE: 9.06. Test Global MAPE: 57.52\n",
            "Number of Clusters = num_clusters_8: Train Global MAPE: 8.35. Test Global MAPE: 34.02\n",
            "Number of Clusters = num_clusters_9: Train Global MAPE: 10.0. Test Global MAPE: 34.06\n",
            "Number of Clusters = num_clusters_10: Train Global MAPE: 9.84. Test Global MAPE: 130.70999999999998\n",
            "Number of Clusters = num_clusters_11: Train Global MAPE: 9.45. Test Global MAPE: 174.34\n",
            "Number of Clusters = num_clusters_12: Train Global MAPE: 9.1. Test Global MAPE: 259.79\n",
            "Number of Clusters = num_clusters_13: Train Global MAPE: 10.71. Test Global MAPE: 259.95\n",
            "Number of Clusters = num_clusters_14: Train Global MAPE: 10.45. Test Global MAPE: 133.87\n",
            "Number of Clusters = num_clusters_15: Train Global MAPE: 10.05. Test Global MAPE: 132.95999999999998\n",
            "Number of Clusters = num_clusters_16: Train Global MAPE: 10.69. Test Global MAPE: 77.68\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "snNHe8YWqkk6",
        "colab_type": "text"
      },
      "source": [
        "### **3. Saving results to BigQuery**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUbvd7hWqzue",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3527e3b9-f077-4fd1-c552-20ea44b1000a"
      },
      "source": [
        "for cluster in agglo_results:\n",
        "    global_train = agglo_results[cluster]['global_train']\n",
        "    global_test = agglo_results[cluster]['global_test']\n",
        "\n",
        "    global_train['cluster_type'] = \"agglo\"\n",
        "    global_train['num_clusters'] = cluster \n",
        "    global_test['cluster_type'] = \"agglo\"\n",
        "    global_test['num_clusters'] = cluster\n",
        "\n",
        "    output_to_bq(global_train, table_id='clusters_20200802.agglo_16_clusters_train_results')\n",
        "    output_to_bq(global_test, table_id='clusters_20200802.agglo_16_clusters_test_results')\n",
        "    print(f\"{cluster} results uploaded to BQ!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "23040 out of 23040 rows loaded.\n",
            "1it [00:05,  5.07s/it]\n",
            "2688 out of 2688 rows loaded.\n",
            "1it [00:02,  2.78s/it]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "num_clusters_2 results uploaded to BQ!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "51840 out of 51840 rows loaded.\n",
            "1it [00:06,  6.28s/it]\n",
            "4032 out of 4032 rows loaded.\n",
            "1it [00:05,  5.96s/it]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "num_clusters_3 results uploaded to BQ!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "92160 out of 92160 rows loaded.\n",
            "1it [00:07,  7.69s/it]\n",
            "5376 out of 5376 rows loaded.\n",
            "1it [00:04,  4.40s/it]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "num_clusters_4 results uploaded to BQ!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "144000 out of 144000 rows loaded.\n",
            "1it [00:12, 12.48s/it]\n",
            "6720 out of 6720 rows loaded.\n",
            "1it [00:01,  1.96s/it]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "num_clusters_5 results uploaded to BQ!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "207360 out of 207360 rows loaded.\n",
            "1it [00:22, 23.00s/it]\n",
            "8064 out of 8064 rows loaded.\n",
            "1it [00:03,  3.59s/it]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "num_clusters_6 results uploaded to BQ!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "282240 out of 282240 rows loaded.\n",
            "1it [00:14, 14.85s/it]\n",
            "9408 out of 9408 rows loaded.\n",
            "1it [00:05,  5.02s/it]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "num_clusters_7 results uploaded to BQ!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "368640 out of 368640 rows loaded.\n",
            "1it [00:17, 17.89s/it]\n",
            "10752 out of 10752 rows loaded.\n",
            "1it [00:02,  2.80s/it]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "num_clusters_8 results uploaded to BQ!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "466560 out of 466560 rows loaded.\n",
            "1it [00:23, 23.71s/it]\n",
            "12096 out of 12096 rows loaded.\n",
            "1it [00:06,  6.48s/it]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "num_clusters_9 results uploaded to BQ!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "576000 out of 576000 rows loaded.\n",
            "1it [00:28, 28.83s/it]\n",
            "13440 out of 13440 rows loaded.\n",
            "1it [00:05,  5.44s/it]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "num_clusters_10 results uploaded to BQ!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "696960 out of 696960 rows loaded.\n",
            "1it [00:54, 54.07s/it]\n",
            "14784 out of 14784 rows loaded.\n",
            "1it [00:04,  4.42s/it]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "num_clusters_11 results uploaded to BQ!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "829440 out of 829440 rows loaded.\n",
            "1it [00:46, 46.52s/it]\n",
            "16128 out of 16128 rows loaded.\n",
            "1it [00:03,  3.51s/it]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "num_clusters_12 results uploaded to BQ!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "973440 out of 973440 rows loaded.\n",
            "1it [00:42, 42.88s/it]\n",
            "17472 out of 17472 rows loaded.\n",
            "1it [00:02,  2.66s/it]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "num_clusters_13 results uploaded to BQ!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1128960 out of 1128960 rows loaded.\n",
            "1it [00:49, 49.28s/it]\n",
            "18816 out of 18816 rows loaded.\n",
            "1it [00:02,  2.62s/it]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "num_clusters_14 results uploaded to BQ!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1296000 out of 1296000 rows loaded.\n",
            "1it [00:55, 55.39s/it]\n",
            "20160 out of 20160 rows loaded.\n",
            "1it [00:03,  3.57s/it]\n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "num_clusters_15 results uploaded to BQ!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1474560 out of 1474560 rows loaded.\n",
            "1it [01:07, 67.60s/it]\n",
            "21504 out of 21504 rows loaded.\n",
            "1it [00:02,  2.78s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "num_clusters_16 results uploaded to BQ!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5zYpYf6OAZ7",
        "colab_type": "text"
      },
      "source": [
        "# 3Ô∏è‚É£ **SOM clusters on a different Train/Test Split**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PPCqKwxPWQb",
        "colab_type": "text"
      },
      "source": [
        "### **1. Generating New Train/Test Split**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9grWwuTBOJet",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bigquery --project machine-learning-msc lcl_ids --use_bqstorage_api\n",
        "SELECT \n",
        "DISTINCT lcl_id, 'test' AS train_test_split, 'original_split' AS random_state\n",
        "FROM `machine-learning-msc.forecasting_20200719.test_set`\n",
        "UNION ALL \n",
        "SELECT \n",
        "DISTINCT lcl_id, 'train' AS train_test_split, 'original_split' AS random_state\n",
        "FROM `machine-learning-msc.forecasting_20200719.train_set`"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t_wPxlvdOrFa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_state_100, test_state_100 = train_test_split(lcl_ids, test_size=0.2716653083, random_state=100)\n",
        "train_state_500, test_state_500 = train_test_split(lcl_ids, test_size=0.2716653083, random_state=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbB9OlYuQLGX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_train_test_set(train_df, test_df, random_state_str):\n",
        "    train = train_df.copy()\n",
        "    test = test_df.copy()\n",
        "\n",
        "    train['train_test_split'] = 'train'\n",
        "    train['random_state'] = random_state_str\n",
        "    test['train_test_split'] = 'test'\n",
        "    test['random_state'] = random_state_str\n",
        "\n",
        "    all = pd.concat([train, test])\n",
        "    assert len(all) == 3681\n",
        "\n",
        "    output_to_bq(all, table_id='households.train_test_split')\n",
        "    print(\"Data uploaded to BigQuery! üèÑ\")\n",
        "    return all"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLG45H5IQoZJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        },
        "outputId": "ec9c2bb9-8fe8-42b1-e3d5-15664a05ed32"
      },
      "source": [
        "create_train_test_set(train_state_100, test_state_100, '100')\n",
        "create_train_test_set(train_state_500, test_state_500, '500')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1it [00:04,  4.24s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Data uploaded to BigQuery! üèÑ\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1it [00:03,  3.68s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Data uploaded to BigQuery! üèÑ\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lcl_id</th>\n",
              "      <th>train_test_split</th>\n",
              "      <th>random_state</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1353</th>\n",
              "      <td>MAC005213</td>\n",
              "      <td>train</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3230</th>\n",
              "      <td>MAC004168</td>\n",
              "      <td>train</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>369</th>\n",
              "      <td>MAC001630</td>\n",
              "      <td>train</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>914</th>\n",
              "      <td>MAC002033</td>\n",
              "      <td>train</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>848</th>\n",
              "      <td>MAC002600</td>\n",
              "      <td>train</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1375</th>\n",
              "      <td>MAC005041</td>\n",
              "      <td>test</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3550</th>\n",
              "      <td>MAC002527</td>\n",
              "      <td>test</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1127</th>\n",
              "      <td>MAC000264</td>\n",
              "      <td>test</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2964</th>\n",
              "      <td>MAC002423</td>\n",
              "      <td>test</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>258</th>\n",
              "      <td>MAC000950</td>\n",
              "      <td>test</td>\n",
              "      <td>500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3681 rows √ó 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         lcl_id train_test_split random_state\n",
              "1353  MAC005213            train          500\n",
              "3230  MAC004168            train          500\n",
              "369   MAC001630            train          500\n",
              "914   MAC002033            train          500\n",
              "848   MAC002600            train          500\n",
              "...         ...              ...          ...\n",
              "1375  MAC005041             test          500\n",
              "3550  MAC002527             test          500\n",
              "1127  MAC000264             test          500\n",
              "2964  MAC002423             test          500\n",
              "258   MAC000950             test          500\n",
              "\n",
              "[3681 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip5g2lwoSQ5p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6d895eb7-cc05-4dae-9d45-2c46d040a765"
      },
      "source": [
        "output_to_bq(lcl_ids, table_id='households.train_test_split')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1it [00:04,  4.62s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlGgoCBmTEVc",
        "colab_type": "text"
      },
      "source": [
        "### **2. Training and Evaluating SOM Clusters**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZZeXuJ3VuRn",
        "colab_type": "text"
      },
      "source": [
        "##### **Downloading Data from BQ**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5bYYIqUTHQA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bigquery --project machine-learning-msc df_100 --use_bqstorage_api\n",
        "WITH \n",
        "raw_data AS (\n",
        "    SELECT * FROM `machine-learning-msc.forecasting_20200719.train_set` data \n",
        "    UNION ALL \n",
        "    SELECT * FROM `machine-learning-msc.forecasting_20200719.test_set` data \n",
        "    ),\n",
        "\n",
        "stg1 AS (\n",
        "    SELECT \n",
        "    data.lcl_id,\n",
        "    split.train_test_split,\n",
        "    FORMAT_DATETIME(\"%B\", DATETIME(data.ts)) AS month_name,\n",
        "    data.dayofweek,\n",
        "    data.hhourly_rank,\n",
        "    ROUND(AVG(data.kwhh),4) AS hh_avg\n",
        "\n",
        "    FROM raw_data data\n",
        "\n",
        "    INNER JOIN `machine-learning-msc.households.train_test_split` split \n",
        "        ON split.lcl_id = data.lcl_id \n",
        "\n",
        "    WHERE data.ts >= '2012-11-01' AND data.ts < '2013-03-01'\n",
        "    AND split.random_state = '100'\n",
        "\n",
        "    GROUP BY 1,2,3,4,5\n",
        "    )\n",
        "\n",
        "SELECT \n",
        "*,\n",
        "ROW_NUMBER() OVER (PARTITION BY lcl_id, month_name ORDER BY dayofweek ASC, hhourly_rank ASC) AS weekly_rank\n",
        "FROM stg1 \n",
        "ORDER BY lcl_id, month_name, weekly_rank, hhourly_rank"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrwnQlU2U_Zq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bigquery --project machine-learning-msc df_500 --use_bqstorage_api\n",
        "WITH \n",
        "raw_data AS (\n",
        "    SELECT * FROM `machine-learning-msc.forecasting_20200719.train_set` data \n",
        "    UNION ALL \n",
        "    SELECT * FROM `machine-learning-msc.forecasting_20200719.test_set` data \n",
        "    ),\n",
        "\n",
        "stg1 AS (\n",
        "    SELECT \n",
        "    data.lcl_id,\n",
        "    split.train_test_split,\n",
        "    FORMAT_DATETIME(\"%B\", DATETIME(data.ts)) AS month_name,\n",
        "    data.dayofweek,\n",
        "    data.hhourly_rank,\n",
        "    ROUND(AVG(data.kwhh),4) AS hh_avg\n",
        "\n",
        "    FROM raw_data data\n",
        "\n",
        "    INNER JOIN `machine-learning-msc.households.train_test_split` split \n",
        "        ON split.lcl_id = data.lcl_id \n",
        "\n",
        "    WHERE data.ts >= '2012-11-01' AND data.ts < '2013-03-01'\n",
        "    AND split.random_state = '500'\n",
        "\n",
        "    GROUP BY 1,2,3,4,5\n",
        "    )\n",
        "\n",
        "SELECT \n",
        "*,\n",
        "ROW_NUMBER() OVER (PARTITION BY lcl_id, month_name ORDER BY dayofweek ASC, hhourly_rank ASC) AS weekly_rank\n",
        "FROM stg1 \n",
        "ORDER BY lcl_id, month_name, weekly_rank, hhourly_rank"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SDwidAQUgA2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_100_train = df_100.query('train_test_split==\"train\"')\n",
        "df_100_test = df_100.query('train_test_split==\"test\"')\n",
        "\n",
        "df_500_train = df_500.query('train_test_split==\"train\"')\n",
        "df_500_test = df_500.query('train_test_split==\"test\"')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "brQ3sgehVRRA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e41aa99c-d6c7-465e-c5ab-f9ace93c32b8"
      },
      "source": [
        "df_100_train['lcl_id'].nunique(), df_100_test['lcl_id'].nunique(), df_500_train['lcl_id'].nunique(), df_500_test['lcl_id'].nunique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2681, 1000, 2681, 1000)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTmeCjcSVy5e",
        "colab_type": "text"
      },
      "source": [
        "##### **Normalising Data Set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HcrmSn4VSYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "value_list = ['hh_avg']\n",
        "column_list = ['month_name', 'weekly_rank']\n",
        "\n",
        "normaliser = Normaliser(value_list, column_list)\n",
        "norm_df_100_train = normaliser.fit(df_100_train)\n",
        "norm_df_100_test = normaliser.predict(df_100_test)\n",
        "norm_df_500_train = normaliser.fit(df_500_train)\n",
        "norm_df_500_test = normaliser.predict(df_500_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggmITtYHWPOJ",
        "colab_type": "text"
      },
      "source": [
        "##### **Training SOM Clusters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxF4891wWMzf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_clusters(random_state_str, cluster_num, train_df, test_df):\n",
        "    som = TrainClusters(cluster_type=\"som\")\n",
        "    som.fit(train_df, cluster_num=cluster_num, sigma=0.1, learning_rate=0.1)\n",
        "    train_pred = som.predict(train_df)\n",
        "    test_pred = som.predict(test_df)\n",
        "\n",
        "    train_pred['train_test_split'] = 'train'\n",
        "    test_pred['train_test_split'] = 'test'\n",
        "\n",
        "    results = pd.concat([train_pred[['lcl_id','cluster','train_test_split']],\n",
        "                         test_pred[['lcl_id','cluster','train_test_split']]])\n",
        "    \n",
        "    results['num_clusters'] = cluster_num\n",
        "    results['random_state'] = random_state_str\n",
        "    results['cluster_type'] = \"som\"\n",
        "    results['date'] = \"2020-08-02\"\n",
        "\n",
        "    output_to_bq(results, 'clusters.clusters')\n",
        "    print(\"Results uploaded to BQ! üèµ\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WdOSHGH5Xnie",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "69b40bc2-22a1-4153-8c8d-4909151cac9e"
      },
      "source": [
        "train_clusters(\"100\", 5, norm_df_100_train, norm_df_100_test)\n",
        "train_clusters(\"500\", 5, norm_df_500_train, norm_df_500_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " [ 100000 / 100000 ] 100% - 0:00:00 left \n",
            " quantization error: 19.462641192440024\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2681it [00:01, 2143.68it/s]\n",
            "1000it [00:00, 2093.45it/s]\n",
            "1it [00:06,  6.15s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Results uploaded to BQ! üèµ\n",
            " [ 100000 / 100000 ] 100% - 0:00:00 left "
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " quantization error: 19.391452420983683\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2681it [00:01, 2201.93it/s]\n",
            "1000it [00:00, 2191.30it/s]\n",
            "1it [00:10, 10.47s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Results uploaded to BQ! üèµ\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BStbm8nXlKHh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bigquery --project machine-learning-msc original_clusters --use_bqstorage_api\n",
        "\n",
        "SELECT \n",
        "lcl_id,\n",
        "cluster,\n",
        "train_test_split,\n",
        "num_clusters,\n",
        "\"original_split\" AS random_state,\n",
        "cluster_type,\n",
        "\"2020-07-30\" AS date\n",
        "FROM `machine-learning-msc.clusters_20200739.clusters`\n",
        "WHERE cluster_type = 'som' AND num_clusters = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cntWBQcelT0y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "6e4f8b52-f462-493e-a38c-a2adc539fdfb"
      },
      "source": [
        "original_clusters.dtypes"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "lcl_id               object\n",
              "cluster             float64\n",
              "train_test_split     object\n",
              "num_clusters          int64\n",
              "random_state         object\n",
              "cluster_type         object\n",
              "date                 object\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 180
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjF6U9jglQrw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6200cca1-26b3-445f-8a5e-253461c2e075"
      },
      "source": [
        "output_to_bq(original_clusters, 'clusters.clusters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3681 out of 3681 rows loaded.\n",
            "1it [00:05,  5.13s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBwToKIyZTpB",
        "colab_type": "text"
      },
      "source": [
        "##### **Evaluating Clusters: Download Timeseries From BQ**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IC_TUyDX4ft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bigquery --project machine-learning-msc df_time_series --use_bqstorage_api\n",
        "\n",
        "WITH all_data AS (\n",
        "    SELECT * FROM `machine-learning-msc.forecasting_20200719.test_set`\n",
        "    UNION ALL \n",
        "    SELECT * FROM `machine-learning-msc.forecasting_20200719.train_set`   \n",
        "    )\n",
        "\n",
        "SELECT \n",
        "data.lcl_id,\n",
        "data.ts AS ds,\n",
        "data.kwhh AS y,\n",
        "weather.air_temperature\n",
        "\n",
        "FROM all_data data\n",
        "LEFT JOIN `machine-learning-msc.london_heathrow_hourly_weather_data.london_heathrow_hourly_weather` weather \n",
        "  ON TIMESTAMP_TRUNC(weather.ts, HOUR) = TIMESTAMP_TRUNC(data.ts, hour)\n",
        "WHERE data.ts >= '2012-11-01' AND data.ts < '2013-03-01'\n",
        "ORDER BY 1,2 ASC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3b1UEjuczWj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_time_series['ds'] = df_time_series['ds'].dt.tz_localize(None) # remove timezones"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-1PEqmxaf50",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bigquery --project machine-learning-msc df_clusters_random_state_100 --use_bqstorage_api\n",
        "SELECT \n",
        "* \n",
        "FROM `machine-learning-msc.clusters.clusters` \n",
        "WHERE random_state = '100' AND date = \"2020-08-02\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_4Lna5ma5nz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bigquery --project machine-learning-msc df_clusters_random_state_500 --use_bqstorage_api\n",
        "SELECT \n",
        "* \n",
        "FROM `machine-learning-msc.clusters.clusters` \n",
        "WHERE random_state = '500' AND date = \"2020-08-02\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeAojMpXbGoi",
        "colab_type": "text"
      },
      "source": [
        "##### **Evaluating Clusters: Training Forecasts**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIyANAbLa7Ya",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_clusters(df_train, df_test, test_period=\"2013-02-01\"):\n",
        "    forecast_dict = {}\n",
        "    test_global_fc = pd.DataFrame()\n",
        "    train_global_fc = pd.DataFrame()\n",
        "    clusters = df_train.groupby('cluster').count().index.to_list()\n",
        "    for cluster in clusters:\n",
        "        cluster_dict = {} \n",
        "        print(f\"\\nTraining cluster: {cluster}\") \n",
        "        print(\"---------------------------\")\n",
        "        df_train_cluster = df_train.query(f\"cluster=={cluster}\").copy()\n",
        "        df_test_cluster = df_test.query(f\"cluster=={cluster}\").copy()\n",
        "        model = TrainProphet(test_period)\n",
        "        model.fit(df_train_cluster)\n",
        "        model.evaluate_test_global_mape(df_test_cluster)\n",
        "        cluster_dict['model'] = model \n",
        "        forecast_dict[f'cluster_{cluster}']=cluster_dict\n",
        "        test_global_fc = pd.concat([test_global_fc, model.test_forecast])\n",
        "\n",
        "        train_forecast = df_train[['cluster','ds','y']].copy()\n",
        "        train_forecast['max_households'] = df_train['households_num'].max()\n",
        "        train_forecast = train_forecast.merge(model.forecast[['ds', 'yhat']], left_on='ds', right_on='ds')\n",
        "        train_forecast['y_global'] = train_forecast['y'] * train_forecast['max_households']\n",
        "        train_forecast['yhat_global'] = train_forecast['yhat'] * train_forecast['max_households']\n",
        "        train_global_fc = pd.concat([train_global_fc, train_forecast])\n",
        "\n",
        "    return forecast_dict, test_global_fc, train_global_fc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QzySntXbSlr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_timeseries(df, clusters_df, cluster_type, num_clusters):\n",
        "    clusters = clusters_df.query(f\"cluster_type=='{cluster_type}' and num_clusters=={num_clusters}\")\n",
        "    out_df = df.merge(clusters[['lcl_id','cluster']], left_on='lcl_id', right_on='lcl_id')\n",
        "\n",
        "    assert out_df['lcl_id'].nunique() == clusters['lcl_id'].nunique()\n",
        "    \n",
        "    households_num = pd.DataFrame(out_df.groupby('cluster')['lcl_id'].nunique())\n",
        "    households_num.rename(columns={'lcl_id':'households_num'}, inplace=True)\n",
        "\n",
        "    timeseries = out_df.groupby(['cluster','ds']).mean().reset_index()\n",
        "    timeseries = timeseries.merge(households_num, left_on='cluster', right_on='cluster')\n",
        "    return timeseries"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGhoB78fe4yu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_results(global_train, global_test):\n",
        "    train = global_train.groupby('ds')[['y_global', 'yhat_global']].sum()\n",
        "    test = global_test.groupby('ds')[['y_global', 'yhat_global']].sum()\n",
        "\n",
        "    train_mape = np.round(np.mean(np.abs(train['yhat_global']/train['y_global']-1)),4)*100\n",
        "    test_mape = np.round(np.mean(np.abs(test['yhat_global']/test['y_global']-1)),4)*100\n",
        "\n",
        "    print(f\"Train global MAPE: {train_mape}. Test global MAPE: {test_mape}.\")\n",
        "    return train_mape, test_mape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okENxb1objAq",
        "colab_type": "text"
      },
      "source": [
        "##### Random State 100 Train Test Splits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWycUWI8bX17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_100 = get_timeseries(df_time_series, df_clusters_random_state_100.query(\"train_test_split=='train'\"), cluster_type=\"som\", num_clusters=5)\n",
        "test_100 = get_timeseries(df_time_series, df_clusters_random_state_100.query(\"train_test_split=='test'\"), cluster_type=\"som\", num_clusters=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6FKblglcQhh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "308a3474-a0d6-4c6f-d7e1-fec1feb92481"
      },
      "source": [
        "model_dict, global_test_100, global_train_100 = train_clusters(train_100, test_100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training cluster: 0.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 8.749568452380938\n",
            "Test Mean Absolute Percentage Error: 10.67\n",
            "\n",
            "Training cluster: 1.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 9.242745535714276\n",
            "Test Mean Absolute Percentage Error: 16.1\n",
            "\n",
            "Training cluster: 2.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 6.979308035714286\n",
            "Test Mean Absolute Percentage Error: 7.84\n",
            "\n",
            "Training cluster: 3.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 66.5159077380953\n",
            "Test Mean Absolute Percentage Error: 74.42999999999999\n",
            "\n",
            "Training cluster: 4.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 8.665863095238118\n",
            "Test Mean Absolute Percentage Error: 10.61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elp8rDswfLmn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ba4dd6c7-3e03-487f-9e1b-07f4b277e790"
      },
      "source": [
        "evaluate_results(global_train_100, global_test_100)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train global MAPE: 9.64. Test global MAPE: 7.93.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Za8CMtVgFZx",
        "colab_type": "text"
      },
      "source": [
        "##### Random State 500 Train Test Splits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5uhgTWnNgGzX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "7bc2107c-ce00-4b5b-e6ca-ea962d1bd496"
      },
      "source": [
        "train_500 = get_timeseries(df_time_series, df_clusters_random_state_500.query(\"train_test_split=='train'\"), cluster_type=\"som\", num_clusters=5)\n",
        "test_500 = get_timeseries(df_time_series, df_clusters_random_state_500.query(\"train_test_split=='test'\"), cluster_type=\"som\", num_clusters=5)\n",
        "model_dict, global_test_500, global_train_500 = train_clusters(train_500, test_500)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training cluster: 0.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 8.693913690476197\n",
            "Test Mean Absolute Percentage Error: 11.87\n",
            "\n",
            "Training cluster: 1.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 10.70595982142858\n",
            "Test Mean Absolute Percentage Error: 16.76\n",
            "\n",
            "Training cluster: 2.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 7.108831845238102\n",
            "Test Mean Absolute Percentage Error: 8.1\n",
            "\n",
            "Training cluster: 3.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 9.402187500000014\n",
            "Test Mean Absolute Percentage Error: 9.45\n",
            "\n",
            "Training cluster: 4.0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 69.7506994047619\n",
            "Test Mean Absolute Percentage Error: 50.39\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AG_EZzD5gto9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fa431149-3438-40cb-d784-80fd37e2d46e"
      },
      "source": [
        "evaluate_results(global_test_500, global_train_500)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train global MAPE: 7.3. Test global MAPE: 9.049999999999999.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2u0ZYwJ517A",
        "colab_type": "text"
      },
      "source": [
        "# 4Ô∏è‚É£ **Using 2012 Winter to predict 2013 winter**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D12ZW11l7RWd",
        "colab_type": "text"
      },
      "source": [
        "### **‚è¨ Downloading data from BQ**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qV2Ubwtb8491",
        "colab_type": "text"
      },
      "source": [
        "##### Train test splits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n9BpfEN5-Xb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bigquery --project machine-learning-msc df_train_test_splits --use_bqstorage_api\n",
        "SELECT * FROM `machine-learning-msc.households.train_test_split` "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vVEXe71871t",
        "colab_type": "text"
      },
      "source": [
        "##### Timeseries forecast inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_k8ygYm6A1f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bigquery --project machine-learning-msc df_2012_timeseries --use_bqstorage_api\n",
        "\n",
        "WITH all_data AS (\n",
        "    SELECT * FROM `machine-learning-msc.forecasting_20200719.test_set`\n",
        "    UNION ALL \n",
        "    SELECT * FROM `machine-learning-msc.forecasting_20200719.train_set`   \n",
        "    )\n",
        "\n",
        "SELECT\n",
        "data.lcl_id,\n",
        "data.ts AS ds,\n",
        "data.kwhh AS y,\n",
        "weather.air_temperature\n",
        "\n",
        "FROM all_data data\n",
        "LEFT JOIN `machine-learning-msc.london_heathrow_hourly_weather_data.london_heathrow_hourly_weather` weather \n",
        "  ON TIMESTAMP_TRUNC(weather.ts, HOUR) = TIMESTAMP_TRUNC(data.ts, hour)\n",
        "WHERE data.ts >= '2012-11-01' AND data.ts < '2013-03-01'\n",
        "ORDER BY 1,2 ASC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38C1NabF6Zuy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bigquery --project machine-learning-msc df_2013_timeseries --use_bqstorage_api\n",
        "\n",
        "WITH all_data AS (\n",
        "    SELECT * FROM `machine-learning-msc.forecasting_20200719.test_set`\n",
        "    UNION ALL \n",
        "    SELECT * FROM `machine-learning-msc.forecasting_20200719.train_set`   \n",
        "    )\n",
        "\n",
        "SELECT\n",
        "data.lcl_id,\n",
        "data.ts AS ds,\n",
        "data.kwhh AS y,\n",
        "weather.air_temperature\n",
        "\n",
        "FROM all_data data\n",
        "LEFT JOIN `machine-learning-msc.london_heathrow_hourly_weather_data.london_heathrow_hourly_weather` weather \n",
        "  ON TIMESTAMP_ADD(TIMESTAMP_TRUNC(weather.ts, HOUR), INTERVAL 365 DAY) = TIMESTAMP_TRUNC(data.ts, hour)\n",
        "WHERE data.ts >= '2013-11-01' AND data.ts < '2014-03-01'\n",
        "ORDER BY 1,2 ASC"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEZL9-8X6kB2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_2012_timeseries['ds'] = df_2012_timeseries['ds'].dt.tz_localize(None) # remove timezones\n",
        "df_2013_timeseries['ds'] = df_2013_timeseries['ds'].dt.tz_localize(None) # remove timezones"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3b5z8K-9B0K",
        "colab_type": "text"
      },
      "source": [
        "##### Cluster Inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXv3-beh8Ghy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bigquery --project machine-learning-msc df_2012_aggregated_ts --use_bqstorage_api\n",
        "WITH \n",
        "raw_data AS (\n",
        "    SELECT * FROM `machine-learning-msc.forecasting_20200719.train_set` data \n",
        "    UNION ALL \n",
        "    SELECT * FROM `machine-learning-msc.forecasting_20200719.test_set` data \n",
        "    ),\n",
        "stg1 AS (\n",
        "    SELECT \n",
        "    data.lcl_id,\n",
        "    FORMAT_DATETIME(\"%B\", DATETIME(data.ts)) AS month_name,\n",
        "    data.dayofweek,\n",
        "    data.hhourly_rank,\n",
        "    ROUND(AVG(data.kwhh),4) AS hh_avg\n",
        "\n",
        "    FROM raw_data data\n",
        "    WHERE data.ts >= '2012-11-01' AND data.ts < '2013-03-01'\n",
        "    GROUP BY 1,2,3,4\n",
        "    )\n",
        "\n",
        "SELECT \n",
        "*,\n",
        "ROW_NUMBER() OVER (PARTITION BY lcl_id, month_name ORDER BY dayofweek ASC, hhourly_rank ASC) AS weekly_rank\n",
        "FROM stg1 \n",
        "ORDER BY lcl_id, month_name, weekly_rank, hhourly_rank"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdxPO8Bg8VI5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bigquery --project machine-learning-msc df_2013_aggregated_ts --use_bqstorage_api\n",
        "WITH \n",
        "raw_data AS (\n",
        "    SELECT * FROM `machine-learning-msc.forecasting_20200719.train_set` data \n",
        "    UNION ALL \n",
        "    SELECT * FROM `machine-learning-msc.forecasting_20200719.test_set` data \n",
        "    ),\n",
        "stg1 AS (\n",
        "    SELECT \n",
        "    data.lcl_id,\n",
        "    FORMAT_DATETIME(\"%B\", DATETIME(data.ts)) AS month_name,\n",
        "    data.dayofweek,\n",
        "    data.hhourly_rank,\n",
        "    ROUND(AVG(data.kwhh),4) AS hh_avg\n",
        "\n",
        "    FROM raw_data data\n",
        "    WHERE data.ts >= '2013-11-01' AND data.ts < '2014-03-01'\n",
        "    GROUP BY 1,2,3,4\n",
        "    )\n",
        "\n",
        "SELECT \n",
        "*,\n",
        "ROW_NUMBER() OVER (PARTITION BY lcl_id, month_name ORDER BY dayofweek ASC, hhourly_rank ASC) AS weekly_rank\n",
        "FROM stg1 \n",
        "ORDER BY lcl_id, month_name, weekly_rank, hhourly_rank"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XzQnhksr8Y-s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "07a8ab87-f8a6-4cae-e7d4-86cb9b67a565"
      },
      "source": [
        "print(len(df_2013_aggregated_ts), len(df_2012_aggregated_ts), len(df_2013_timeseries), len(df_2012_timeseries))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4841157 4947264 20468887 21197743\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-m80UY07dK7",
        "colab_type": "text"
      },
      "source": [
        "### **üß™ Create Train and Test set data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqd5swRn7cqn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_inputs(random_state, df_train_test_splits, df_2012, df_2013):\n",
        "    # Train set : 2012 data \n",
        "    train_df = df_2012.merge(\n",
        "        df_train_test_splits.query(f\"random_state=='{random_state}' and train_test_split=='train'\"),\n",
        "        left_on='lcl_id',\n",
        "        right_on='lcl_id',\n",
        "        how='inner'\n",
        "        )\n",
        "    # Test set: 2013 data\n",
        "    test_df = df_2013.merge(\n",
        "        df_train_test_splits.query(f\"random_state=='{random_state}' and train_test_split=='test'\"),\n",
        "        left_on='lcl_id',\n",
        "        right_on='lcl_id',\n",
        "        how='inner'\n",
        "        )\n",
        "\n",
        "    assert train_df['lcl_id'].nunique() == 2681\n",
        "    assert test_df['lcl_id'].nunique() < train_df['lcl_id'].nunique()\n",
        "    return train_df, test_df"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jefp5EiyBSmy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_timeseries_inputs(random_state, df_cluster, df_ts):\n",
        "  \n",
        "    df_forecast = df_ts.merge(\n",
        "        df_cluster,\n",
        "        left_on='lcl_id',\n",
        "        right_on='lcl_id',\n",
        "        how='inner'\n",
        "        )\n",
        "    \n",
        "    households_num = pd.DataFrame(df_forecast.groupby('cluster')['lcl_id'].nunique())\n",
        "    households_num.rename(columns={'lcl_id':'households_num'}, inplace=True)\n",
        "    households_num.reset_index(inplace=True)\n",
        "\n",
        "    timeseries = df_forecast.groupby(['cluster','ds']).mean().reset_index()\n",
        "    timeseries = timeseries.merge(households_num, left_on='cluster', right_on='cluster')\n",
        "\n",
        "    return timeseries"
      ],
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMM523FANH1Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_results(global_train, global_test):\n",
        "    train = global_train.groupby('ds')[['y_global', 'yhat_global']].sum()\n",
        "    test = global_test.groupby('ds')[['y_global', 'yhat_global']].sum()\n",
        "\n",
        "    train_mape = np.round(np.mean(np.abs(train['yhat_global']/train['y_global']-1)),4)*100\n",
        "    test_mape = np.round(np.mean(np.abs(test['yhat_global']/test['y_global']-1)),4)*100\n",
        "\n",
        "    print(f\"Train global MAPE: {train_mape}. Test global MAPE: {test_mape}.\")\n",
        "    return train_mape, test_mape"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DnnW5cRG7WhM",
        "colab_type": "text"
      },
      "source": [
        "### **üèãÔ∏è‚Äç‚ôÇÔ∏è Training Clusters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_7TrEMiAQD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_cluster_train, df_cluster_test = get_inputs('original_split', df_train_test_splits, df_2012_aggregated_ts, df_2013_aggregated_ts)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51C2azgP7aZ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d79b2041-db42-48f9-a472-cce7c04c1e1c"
      },
      "source": [
        "value_list = ['hh_avg']\n",
        "column_list = ['month_name', 'weekly_rank']\n",
        "\n",
        "normaliser = Normaliser(value_list, column_list)\n",
        "df_cluster_train_norm = normaliser.fit(df_cluster_train)\n",
        "df_cluster_test_norm = normaliser.predict(df_cluster_test)\n",
        "\n",
        "som = TrainClusters(cluster_type=\"som\")\n",
        "som.fit(df_cluster_train_norm, cluster_num=5, sigma=0.1, learning_rate=0.1)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " [ 100000 / 100000 ] 100% - 0:00:00 left \n",
            " quantization error: 19.84157462314047\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYf8C92S_lF0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "075c942b-62f0-4543-d221-03dd7e80872a"
      },
      "source": [
        "cluster_train_pred = som.predict(df_cluster_train_norm)[['lcl_id', 'cluster']]\n",
        "cluster_test_pred = som.predict(df_cluster_test_norm)[['lcl_id', 'cluster']]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2681it [00:01, 1970.83it/s]\n",
            "996it [00:00, 2019.70it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "amkNG_txAlb0",
        "colab_type": "text"
      },
      "source": [
        "### ü¶æ **Evaluating Clusters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVBZ2YJFAW5x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random_state = 'original_split'\n",
        "df_timeseries_train = get_timeseries_inputs(random_state=random_state,\n",
        "                                            df_train_test_splits=df_train_test_splits,\n",
        "                                            df_cluster=cluster_train_pred,\n",
        "                                            df_ts=df_2012_timeseries)\n",
        "df_timeseries_test  = get_timeseries_inputs(random_state=random_state,\n",
        "                                            df_train_test_splits=df_train_test_splits,\n",
        "                                            df_cluster=cluster_test_pred,\n",
        "                                            df_ts=df_2013_timeseries)"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHbiXOFRA_5R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f8ad8621-4834-4b2e-a9bf-8acc467c05a5"
      },
      "source": [
        "print(len(df_timeseries_train), len(df_timeseries_test))"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "28800 28565\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypzSsBvbO1HH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "d3bb8c44-b5cb-474c-eee8-32819a7ca044"
      },
      "source": [
        "df_timeseries_train"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>cluster</th>\n",
              "      <th>ds</th>\n",
              "      <th>y</th>\n",
              "      <th>air_temperature</th>\n",
              "      <th>households_num</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2012-11-01 00:00:00</td>\n",
              "      <td>1.018714</td>\n",
              "      <td>11.8</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2012-11-01 00:30:00</td>\n",
              "      <td>1.024810</td>\n",
              "      <td>11.8</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2012-11-01 01:00:00</td>\n",
              "      <td>0.914690</td>\n",
              "      <td>8.8</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2012-11-01 01:30:00</td>\n",
              "      <td>0.860905</td>\n",
              "      <td>8.8</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>2012-11-01 02:00:00</td>\n",
              "      <td>0.757500</td>\n",
              "      <td>8.5</td>\n",
              "      <td>42</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28795</th>\n",
              "      <td>4.0</td>\n",
              "      <td>2013-02-28 21:30:00</td>\n",
              "      <td>0.187646</td>\n",
              "      <td>4.0</td>\n",
              "      <td>1460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28796</th>\n",
              "      <td>4.0</td>\n",
              "      <td>2013-02-28 22:00:00</td>\n",
              "      <td>0.178670</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28797</th>\n",
              "      <td>4.0</td>\n",
              "      <td>2013-02-28 22:30:00</td>\n",
              "      <td>0.166942</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28798</th>\n",
              "      <td>4.0</td>\n",
              "      <td>2013-02-28 23:00:00</td>\n",
              "      <td>0.153369</td>\n",
              "      <td>2.7</td>\n",
              "      <td>1460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28799</th>\n",
              "      <td>4.0</td>\n",
              "      <td>2013-02-28 23:30:00</td>\n",
              "      <td>0.131108</td>\n",
              "      <td>2.7</td>\n",
              "      <td>1460</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>28800 rows √ó 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       cluster                  ds         y  air_temperature  households_num\n",
              "0          0.0 2012-11-01 00:00:00  1.018714             11.8              42\n",
              "1          0.0 2012-11-01 00:30:00  1.024810             11.8              42\n",
              "2          0.0 2012-11-01 01:00:00  0.914690              8.8              42\n",
              "3          0.0 2012-11-01 01:30:00  0.860905              8.8              42\n",
              "4          0.0 2012-11-01 02:00:00  0.757500              8.5              42\n",
              "...        ...                 ...       ...              ...             ...\n",
              "28795      4.0 2013-02-28 21:30:00  0.187646              4.0            1460\n",
              "28796      4.0 2013-02-28 22:00:00  0.178670              3.5            1460\n",
              "28797      4.0 2013-02-28 22:30:00  0.166942              3.5            1460\n",
              "28798      4.0 2013-02-28 23:00:00  0.153369              2.7            1460\n",
              "28799      4.0 2013-02-28 23:30:00  0.131108              2.7            1460\n",
              "\n",
              "[28800 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-LmQ0WrGvh_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "fcacfd84-2337-4ee2-fde6-92b1f3f56523"
      },
      "source": [
        "clusters = df_timeseries_train.groupby('cluster').count().index.to_list()\n",
        "\n",
        "train_global_fc = pd.DataFrame()\n",
        "test_global_fc = pd.DataFrame()\n",
        "\n",
        "for cluster in clusters:\n",
        "\n",
        "    train = df_timeseries_train.query(f\"cluster=={cluster}\").copy()\n",
        "    test = df_timeseries_test.query(f\"cluster=={cluster}\").copy()\n",
        "\n",
        "    print(f\"\\nTraining cluster: {cluster}. Number of train households: {train['households_num'].max()}. Number of test households: {test['households_num'].max()}.\")\n",
        "\n",
        "    model = TrainProphet(\"2013-02-01\")\n",
        "    model.fit(train)\n",
        "    model.evaluate_test_global_mape(test, test_period=\"2014-02-01\")\n",
        "    test_global_fc = pd.concat([test_global_fc, model.test_forecast])\n",
        "\n",
        "\n",
        "    train_forecast = train[['cluster','ds','y']].copy()\n",
        "    train_forecast['max_households'] = train['households_num'].max()\n",
        "    train_forecast = train_forecast.merge(model.forecast[['ds', 'yhat']], left_on='ds', right_on='ds')\n",
        "    train_forecast['y_global'] = train_forecast['y'] * train_forecast['max_households']\n",
        "    train_forecast['yhat_global'] = train_forecast['yhat'] * train_forecast['max_households']\n",
        "    train_global_fc = pd.concat([train_global_fc, train_forecast])"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training cluster: 0.0. Number of train households: 42. Number of test households: 40.\n",
            "Training Mean Absolute Percentage Error: 9.670476190476199\n",
            "Test Mean Absolute Percentage Error: 125.61\n",
            "\n",
            "Training cluster: 1.0. Number of train households: 29. Number of test households: 15.\n",
            "Training Mean Absolute Percentage Error: 78.54380208333343\n",
            "Test Mean Absolute Percentage Error: 158.38\n",
            "\n",
            "Training cluster: 2.0. Number of train households: 870. Number of test households: 285.\n",
            "Training Mean Absolute Percentage Error: 9.401599702380938\n",
            "Test Mean Absolute Percentage Error: 9.629999999999999\n",
            "\n",
            "Training cluster: 3.0. Number of train households: 280. Number of test households: 83.\n",
            "Training Mean Absolute Percentage Error: 8.524538690476184\n",
            "Test Mean Absolute Percentage Error: 85.1\n",
            "\n",
            "Training cluster: 4.0. Number of train households: 1460. Number of test households: 573.\n",
            "Training Mean Absolute Percentage Error: 6.989717261904765\n",
            "Test Mean Absolute Percentage Error: 40.739999999999995\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWgZVw06H_mj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "edd14602-2a01-47e4-8ac2-5b729979aaa8"
      },
      "source": [
        "evaluate_results(train_global_fc, test_global_fc)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train global MAPE: 6.97. Test global MAPE: 46.68.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6.97, 46.68)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Nq9hGRqNSfI",
        "colab_type": "text"
      },
      "source": [
        "### ü¶ø**Evaluating Baseline**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLwV0fMYNX3m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_baseline_timeseries_input(random_state, df_2012, df_2013, df_train_test_splits):\n",
        "    \n",
        "    train = df_2012.merge(df_train_test_splits.query(f\"random_state=='{random_state}' and train_test_split=='train'\"),\n",
        "                           left_on='lcl_id',\n",
        "                           right_on='lcl_id',\n",
        "                           how='inner'\n",
        "                            )\n",
        "    \n",
        "    test = df_2013.merge(df_train_test_splits.query(f\"random_state=='{random_state}' and train_test_split=='test'\"),\n",
        "                        left_on='lcl_id',\n",
        "                        right_on='lcl_id',\n",
        "                        how='inner'\n",
        "                        )\n",
        "\n",
        "    train['households_num'] = train['lcl_id'].nunique()\n",
        "    test['households_num'] = test['lcl_id'].nunique()\n",
        "\n",
        "    train = train.groupby('ds').mean().reset_index()\n",
        "    test = test.groupby('ds').mean().reset_index()\n",
        "\n",
        "    assert np.all(train['households_num']==2681)\n",
        "    assert np.all(train['households_num']>test['households_num'].max())\n",
        "    \n",
        "    return train, test"
      ],
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_K1okfSnOVRe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "baseline_train_df, baseline_test_df = get_baseline_timeseries_input('original_split', \n",
        "                                                                    df_2012_timeseries,\n",
        "                                                                    df_2013_timeseries,\n",
        "                                                                    df_train_test_splits)"
      ],
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDV2RzJCNdfH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "4c177735-347d-4f9f-c62b-52a6dcccd5dd"
      },
      "source": [
        "baseline_model = TrainProphet(\"2013-02-01\")\n",
        "baseline_model.fit(baseline_train_df)\n",
        "baseline_model.evaluate_test_global_mape(baseline_test_df, test_period=\"2014-02-01\")"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Mean Absolute Percentage Error: 7.438474702380949\n",
            "Test Mean Absolute Percentage Error: 33.03\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<src.train_prophet.TrainProphet at 0x7f1dc9f44b38>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 130
        }
      ]
    }
  ]
}