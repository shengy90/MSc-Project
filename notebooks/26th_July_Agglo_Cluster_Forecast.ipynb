{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "26th July - Agglo Cluster Forecast",
      "provenance": [],
      "collapsed_sections": [
        "svsqrk45tb_B",
        "3eJpHGVpGuji",
        "NmVtQy38Ozr6",
        "8RuYJx-5tq7J",
        "fgCZb9rPt-7_",
        "81IKcUrPuF0o"
      ],
      "toc_visible": true,
      "mount_file_id": "1XQiLyjIhVdgtFhuzVLHo5QTOzMLa85lV",
      "authorship_tag": "ABX9TyPEKsaxN+qmKKgG8tJ9QCGr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shengy90/MSc-Project/blob/master/notebooks/26th_July_Agglo_Cluster_Forecast.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svsqrk45tb_B",
        "colab_type": "text"
      },
      "source": [
        "# **1Ô∏è‚É£ Setup Notebook üíª**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eJpHGVpGuji",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### **Authenticate with BigQuery ‚òÅÔ∏è**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkaOt64QmU90",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --upgrade google-cloud-bigquery[bqstorage,pandas]\n",
        "!pip install --upgrade pandas-gbq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6y1_cKZGJ1C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d3b05e4b-951e-44f3-c8ea-8c2dbfe65ae5"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "print('Authenticated')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Authenticated\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cPJ-kLQGQ0r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bigquery --project machine-learning-msc df --use_bqstorage_api\n",
        "SELECT \n",
        "  COUNT(*) as total_rows\n",
        "FROM `machine-learning-msc.low_carbon_london.household_consumption_daily_agg` "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ts6qMG3PGlUL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "4cad0fb8-0ef9-4fbe-9087-18219a6e2cbf"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>total_rows</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14841792</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   total_rows\n",
              "0    14841792"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4sbx8kLH89t",
        "colab_type": "text"
      },
      "source": [
        "### **Importing Libraries‚è¨**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmVtQy38Ozr6",
        "colab_type": "text"
      },
      "source": [
        "##### Standard Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0JiDvCG3U4F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "c15cdf87-e361-4fc2-c98a-f1fa7720e71b"
      },
      "source": [
        "!pip install fbprophet"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fbprophet in /usr/local/lib/python3.6/dist-packages (0.6)\n",
            "Requirement already satisfied: Cython>=0.22 in /usr/local/lib/python3.6/dist-packages (from fbprophet) (0.29.21)\n",
            "Requirement already satisfied: cmdstanpy==0.4 in /usr/local/lib/python3.6/dist-packages (from fbprophet) (0.4.0)\n",
            "Requirement already satisfied: pystan>=2.14 in /usr/local/lib/python3.6/dist-packages (from fbprophet) (2.19.1.1)\n",
            "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from fbprophet) (1.18.5)\n",
            "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.6/dist-packages (from fbprophet) (1.0.5)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from fbprophet) (3.2.2)\n",
            "Requirement already satisfied: LunarCalendar>=0.0.9 in /usr/local/lib/python3.6/dist-packages (from fbprophet) (0.0.9)\n",
            "Requirement already satisfied: convertdate>=2.1.2 in /usr/local/lib/python3.6/dist-packages (from fbprophet) (2.2.1)\n",
            "Requirement already satisfied: holidays>=0.9.5 in /usr/local/lib/python3.6/dist-packages (from fbprophet) (0.9.12)\n",
            "Requirement already satisfied: setuptools-git>=1.2 in /usr/local/lib/python3.6/dist-packages (from fbprophet) (1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.0 in /usr/local/lib/python3.6/dist-packages (from fbprophet) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.23.4->fbprophet) (2018.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->fbprophet) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->fbprophet) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.0.0->fbprophet) (1.2.0)\n",
            "Requirement already satisfied: ephem>=3.7.5.3 in /usr/local/lib/python3.6/dist-packages (from LunarCalendar>=0.0.9->fbprophet) (3.7.7.1)\n",
            "Requirement already satisfied: pymeeus<=1,>=0.3.6 in /usr/local/lib/python3.6/dist-packages (from convertdate>=2.1.2->fbprophet) (0.3.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from holidays>=0.9.5->fbprophet) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fr9vUfxAICRD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "2eb7e1eb-85be-4729-faec-b5094127206e"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt \n",
        "import random\n",
        "import datetime as dt\n",
        "\n",
        "from tqdm import tqdm\n",
        "from datetime import date\n",
        "from matplotlib.gridspec import GridSpec\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        " \n",
        "sns.set()\n",
        "%matplotlib inline"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hq2GQCXqO2ZD",
        "colab_type": "text"
      },
      "source": [
        "##### Import Github Repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gw_bhwfTY7U1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "1732f4da-7679-4d5a-830f-3a1348d5e5f3"
      },
      "source": [
        "%cd /content\n",
        "!ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "adc.json  drive  mscproj  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtzIWQvBPESj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "96568bac-d244-42d1-d138-2fc67bd46354"
      },
      "source": [
        "!rm -rf mscproj\n",
        "!git clone https://github.com/shengy90/MSc-Project mscproj\n",
        "!git pull\n",
        "%cd /content/mscproj/\n",
        "!ls"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'mscproj'...\n",
            "remote: Enumerating objects: 306, done.\u001b[K\n",
            "remote: Counting objects: 100% (306/306), done.\u001b[K\n",
            "remote: Compressing objects: 100% (215/215), done.\u001b[K\n",
            "remote: Total 306 (delta 164), reused 204 (delta 83), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (306/306), 10.79 MiB | 12.30 MiB/s, done.\n",
            "Resolving deltas: 100% (164/164), done.\n",
            "fatal: not a git repository (or any of the parent directories): .git\n",
            "/content/mscproj\n",
            "bin\t     __init__.py  notebooks  requirements.txt  sql\n",
            "definitions  Makefile\t  README.md  run.py\t       src\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fnq6lOTUXbat",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%reload_ext autoreload \n",
        "%autoreload 2 \n",
        "from src.train_prophet import TrainProphet"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RuYJx-5tq7J",
        "colab_type": "text"
      },
      "source": [
        "# **2Ô∏è‚É£ Download Datasets from BigQuery**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Gi_uYAbWM9u",
        "colab_type": "text"
      },
      "source": [
        "### **Downloading data for Baseline Model from BQ**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggtyNS1qeWhA",
        "colab_type": "text"
      },
      "source": [
        "##### Baseline model train dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWMdZomqOkfV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bigquery --project machine-learning-msc df_baseline_train --use_bqstorage_api\n",
        "SELECT \n",
        "train.ts AS ds,\n",
        "ROUND(avg(train.kwhh),3) AS y,\n",
        "COUNT(DISTINCT train.lcl_id) AS households_num,\n",
        "MAX(weather.air_temperature) AS air_temperature\n",
        "\n",
        "FROM `machine-learning-msc.forecasting_20200719.train_set` train\n",
        "LEFT JOIN `machine-learning-msc.london_heathrow_hourly_weather_data.london_heathrow_hourly_weather` weather \n",
        "ON TIMESTAMP_TRUNC(weather.ts, HOUR) = TIMESTAMP_TRUNC(train.ts, hour)\n",
        "\n",
        "-- Only include data from November -> February\n",
        "-- We'll use November -> January data to forecast February (28 days X 48 data points a day)\n",
        "WHERE CAST(train.ts AS DATE) >= '2012-11-01' AND  CAST(train.ts AS DATE) < '2013-03-01'\n",
        "\n",
        "GROUP BY 1 \n",
        "ORDER BY 1 ASC"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9eR1EapGuTV4",
        "colab_type": "text"
      },
      "source": [
        "##### Baseline model test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZKg42DTuWNb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bigquery --project machine-learning-msc df_baseline_test --use_bqstorage_api\n",
        "SELECT \n",
        "train.ts AS ds,\n",
        "ROUND(avg(train.kwhh),3) AS y,\n",
        "COUNT(DISTINCT train.lcl_id) AS households_num,\n",
        "MAX(weather.air_temperature) AS air_temperature\n",
        "\n",
        "FROM `machine-learning-msc.forecasting_20200719.test_set` train\n",
        "LEFT JOIN `machine-learning-msc.london_heathrow_hourly_weather_data.london_heathrow_hourly_weather` weather \n",
        "ON TIMESTAMP_TRUNC(weather.ts, HOUR) = TIMESTAMP_TRUNC(train.ts, hour)\n",
        "\n",
        "-- Only include data from November -> February\n",
        "-- We'll use November -> January data to forecast February (28 days X 48 data points a day)\n",
        "WHERE CAST(train.ts AS DATE) >= '2012-11-01' AND  CAST(train.ts AS DATE) < '2013-03-01'\n",
        "\n",
        "GROUP BY 1 \n",
        "ORDER BY 1 ASC"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tTQaA1VFudaz",
        "colab_type": "text"
      },
      "source": [
        "##### Format data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5D0OpnUPU81",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "76a087d5-4e5f-4780-dbb5-461a05ce3994"
      },
      "source": [
        "df_baseline_train['ds'] = df_baseline_train['ds'].dt.tz_localize(None) # remove timezones \n",
        "df_baseline_test['ds'] = df_baseline_test['ds'].dt.tz_localize(None) # remove timezones \n",
        "\n",
        "df_baseline_train.head(), df_baseline_test.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(                   ds      y  households_num  air_temperature\n",
              " 0 2012-11-01 00:00:00  0.185            2680             11.8\n",
              " 1 2012-11-01 00:30:00  0.195            2680             11.8\n",
              " 2 2012-11-01 01:00:00  0.170            2680              8.8\n",
              " 3 2012-11-01 01:30:00  0.154            2680              8.8\n",
              " 4 2012-11-01 02:00:00  0.138            2676              8.5,\n",
              "                    ds      y  households_num  air_temperature\n",
              " 0 2012-11-01 00:00:00  0.189             999             11.8\n",
              " 1 2012-11-01 00:30:00  0.208            1000             11.8\n",
              " 2 2012-11-01 01:00:00  0.185            1000              8.8\n",
              " 3 2012-11-01 01:30:00  0.165            1000              8.8\n",
              " 4 2012-11-01 02:00:00  0.149            1000              8.5)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgCZb9rPt-7_",
        "colab_type": "text"
      },
      "source": [
        "### **Download data for Agglo Clusters from BQ**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b_NVJqYeZ36",
        "colab_type": "text"
      },
      "source": [
        "##### agglo training dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0N52pKPJG_sa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bigquery --project machine-learning-msc df_agglo_train --use_bqstorage_api\n",
        "\n",
        "WITH pca_clusters AS (\n",
        "  SELECT \n",
        "  DISTINCT \n",
        "  lcl_id,\n",
        "  cluster,\n",
        "  cluster_x,\n",
        "  cluster_y\n",
        "  FROM `machine-learning-msc.low_carbon_london.pca_clusters_20200712`\n",
        "  ),\n",
        "  \n",
        "cluster_data AS (\n",
        "  SELECT \n",
        "  pca.cluster,\n",
        "  pca.cluster_x,\n",
        "  pca.cluster_y,\n",
        "  train.*,\n",
        "  weather.air_temperature\n",
        "  FROM `machine-learning-msc.forecasting_20200719.train_set` train\n",
        "  LEFT JOIN pca_clusters pca ON pca.lcl_id = train.lcl_id\n",
        "  LEFT JOIN `machine-learning-msc.london_heathrow_hourly_weather_data.london_heathrow_hourly_weather` weather \n",
        "  ON TIMESTAMP_TRUNC(weather.ts, HOUR) = TIMESTAMP_TRUNC(train.ts, hour)\n",
        "  \n",
        "  WHERE CAST(train.ts AS DATE) >= '2012-11-01' AND  CAST(train.ts AS DATE) < '2013-03-01'\n",
        "  )\n",
        "\n",
        "SELECT \n",
        "cluster,\n",
        "ts AS ds,\n",
        "ROUND(AVG(kwhh),4) AS y,\n",
        "MAX(air_temperature) AS air_temperature,\n",
        "COUNT(DISTINCT lcl_id) AS households_num\n",
        "FROM cluster_data\n",
        "\n",
        "GROUP BY 1,2\n",
        "ORDER BY 1,2"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kej3rr8xvElK",
        "colab_type": "text"
      },
      "source": [
        "##### agglo test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PU5Wkwkuvpp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bigquery --project machine-learning-msc df_agglo_test --use_bqstorage_api\n",
        "\n",
        "WITH pca_clusters AS (\n",
        "  SELECT \n",
        "  DISTINCT \n",
        "  lcl_id,\n",
        "  cluster,\n",
        "  cluster_x,\n",
        "  cluster_y\n",
        "  FROM `machine-learning-msc.low_carbon_london.pca_clusters_20200712`\n",
        "  ),\n",
        "  \n",
        "cluster_data AS (\n",
        "  SELECT \n",
        "  pca.cluster,\n",
        "  pca.cluster_x,\n",
        "  pca.cluster_y,\n",
        "  train.*,\n",
        "  weather.air_temperature\n",
        "  FROM `machine-learning-msc.forecasting_20200719.test_set` train\n",
        "  LEFT JOIN pca_clusters pca ON pca.lcl_id = train.lcl_id\n",
        "  LEFT JOIN `machine-learning-msc.london_heathrow_hourly_weather_data.london_heathrow_hourly_weather` weather \n",
        "  ON TIMESTAMP_TRUNC(weather.ts, HOUR) = TIMESTAMP_TRUNC(train.ts, hour)\n",
        "  \n",
        "  WHERE CAST(train.ts AS DATE) >= '2012-11-01' AND  CAST(train.ts AS DATE) < '2013-03-01'\n",
        "  )\n",
        "\n",
        "SELECT \n",
        "cluster,\n",
        "ts AS ds,\n",
        "ROUND(AVG(kwhh),4) AS y,\n",
        "MAX(air_temperature) AS air_temperature,\n",
        "COUNT(DISTINCT lcl_id) AS households_num\n",
        "FROM cluster_data\n",
        "\n",
        "GROUP BY 1,2\n",
        "ORDER BY 1,2"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E94hR725vGpo",
        "colab_type": "text"
      },
      "source": [
        "##### format data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aC8ofyXyuWab",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "20cb7f76-d9b0-43e0-d110-2191925443bc"
      },
      "source": [
        "df_agglo_train['ds'] = df_agglo_train['ds'].dt.tz_localize(None) # remove timezones \n",
        "df_agglo_test['ds'] = df_agglo_test['ds'].dt.tz_localize(None) # remove timezones \n",
        "\n",
        "df_agglo_train.groupby('ds').mean().head(10), df_agglo_test.groupby('ds').mean().head(10)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(                            y  air_temperature  households_num\n",
              " ds                                                            \n",
              " 2012-11-01 00:00:00  0.610500             11.8      297.777778\n",
              " 2012-11-01 00:30:00  1.130056             11.8      297.777778\n",
              " 2012-11-01 01:00:00  1.022156              8.8      297.777778\n",
              " 2012-11-01 01:30:00  0.934922              8.8      297.777778\n",
              " 2012-11-01 02:00:00  0.780467              8.5      297.333333\n",
              " 2012-11-01 02:30:00  0.681878              8.5      297.666667\n",
              " 2012-11-01 03:00:00  0.636444              7.8      297.777778\n",
              " 2012-11-01 03:30:00  0.617911              7.8      297.777778\n",
              " 2012-11-01 04:00:00  0.583856              7.1      297.777778\n",
              " 2012-11-01 04:30:00  0.549433              7.1      297.777778,\n",
              "                             y  air_temperature  households_num\n",
              " ds                                                            \n",
              " 2012-11-01 00:00:00  0.472189             11.8      111.000000\n",
              " 2012-11-01 00:30:00  0.988578             11.8      111.111111\n",
              " 2012-11-01 01:00:00  0.910100              8.8      111.111111\n",
              " 2012-11-01 01:30:00  0.794933              8.8      111.111111\n",
              " 2012-11-01 02:00:00  0.724589              8.5      111.111111\n",
              " 2012-11-01 02:30:00  0.630456              8.5      111.111111\n",
              " 2012-11-01 03:00:00  0.581967              7.8      111.111111\n",
              " 2012-11-01 03:30:00  0.536189              7.8      111.111111\n",
              " 2012-11-01 04:00:00  0.486400              7.1      111.111111\n",
              " 2012-11-01 04:30:00  0.440644              7.1      111.111111)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81IKcUrPuF0o",
        "colab_type": "text"
      },
      "source": [
        "### **Download data for SOM clusters from BQ**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux3bUX7ned5F",
        "colab_type": "text"
      },
      "source": [
        "##### som training dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaeQHUd0ceGn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bigquery --project machine-learning-msc df_som_train --use_bqstorage_api\n",
        "\n",
        "WITH \n",
        "\n",
        "som_cluster_num AS (\n",
        "  SELECT cluster_x, cluster_y,\n",
        "  CAST(ROW_NUMBER() OVER (ORDER BY cluster_x, cluster_y)-1 AS STRING) AS cluster\n",
        "  FROM `machine-learning-msc.low_carbon_london.som_clusters_20200712`\n",
        "  WHERE train_test_split = 'train'\n",
        "  group by 1,2\n",
        "  order by 1,2 asc\n",
        "  ),\n",
        "  \n",
        "som_clusters AS (\n",
        "  SELECT \n",
        "  DISTINCT \n",
        "  a.lcl_id,\n",
        "  b.cluster,\n",
        "  a.cluster_x,\n",
        "  a.cluster_y\n",
        "  FROM `machine-learning-msc.low_carbon_london.som_clusters_20200712` a \n",
        "  LEFT JOIN som_cluster_num b ON a.cluster_x = b.cluster_x AND a.cluster_y = b.cluster_y\n",
        "  ),\n",
        "\n",
        "cluster_data AS (\n",
        "  SELECT \n",
        "  som.cluster,\n",
        "  som.cluster_x,\n",
        "  som.cluster_y,\n",
        "  train.*,\n",
        "  weather.air_temperature\n",
        "  FROM `machine-learning-msc.forecasting_20200719.train_set` train\n",
        "  LEFT JOIN som_clusters som ON som.lcl_id = train.lcl_id\n",
        "  LEFT JOIN `machine-learning-msc.london_heathrow_hourly_weather_data.london_heathrow_hourly_weather` weather \n",
        "  ON TIMESTAMP_TRUNC(weather.ts, HOUR) = TIMESTAMP_TRUNC(train.ts, hour)\n",
        "  \n",
        "  WHERE CAST(train.ts AS DATE) >= '2012-11-01' AND  CAST(train.ts AS DATE) < '2013-03-01'\n",
        "  )\n",
        "\n",
        "SELECT \n",
        "cluster,\n",
        "ts AS ds,\n",
        "ROUND(AVG(kwhh),4) AS y,\n",
        "ROUND(SUM(kwhh), 4) AS total_kwhh,\n",
        "MAX(air_temperature) AS air_temperature,\n",
        "COUNT(DISTINCT lcl_id) AS households_num\n",
        "FROM cluster_data\n",
        "\n",
        "GROUP BY 1,2\n",
        "ORDER BY 1,2"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mWCq21gvRuc",
        "colab_type": "text"
      },
      "source": [
        "##### som test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6x7KzC2yvOSA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bigquery --project machine-learning-msc df_som_test --use_bqstorage_api\n",
        "\n",
        "WITH \n",
        "\n",
        "som_cluster_num AS (\n",
        "  SELECT cluster_x, cluster_y,\n",
        "  CAST(ROW_NUMBER() OVER (ORDER BY cluster_x, cluster_y)-1 AS STRING) AS cluster\n",
        "  FROM `machine-learning-msc.low_carbon_london.som_clusters_20200712`\n",
        "  WHERE train_test_split = 'test'\n",
        "  group by 1,2\n",
        "  order by 1,2 asc\n",
        "  ),\n",
        "  \n",
        "som_clusters AS (\n",
        "  SELECT \n",
        "  DISTINCT \n",
        "  a.lcl_id,\n",
        "  b.cluster,\n",
        "  a.cluster_x,\n",
        "  a.cluster_y\n",
        "  FROM `machine-learning-msc.low_carbon_london.som_clusters_20200712` a \n",
        "  LEFT JOIN som_cluster_num b ON a.cluster_x = b.cluster_x AND a.cluster_y = b.cluster_y\n",
        "  ),\n",
        "\n",
        "cluster_data AS (\n",
        "  SELECT \n",
        "  som.cluster,\n",
        "  som.cluster_x,\n",
        "  som.cluster_y,\n",
        "  train.*,\n",
        "  weather.air_temperature\n",
        "  FROM `machine-learning-msc.forecasting_20200719.test_set` train\n",
        "  LEFT JOIN som_clusters som ON som.lcl_id = train.lcl_id\n",
        "  LEFT JOIN `machine-learning-msc.london_heathrow_hourly_weather_data.london_heathrow_hourly_weather` weather \n",
        "  ON TIMESTAMP_TRUNC(weather.ts, HOUR) = TIMESTAMP_TRUNC(train.ts, hour)\n",
        "  \n",
        "  WHERE CAST(train.ts AS DATE) >= '2012-11-01' AND  CAST(train.ts AS DATE) < '2013-03-01'\n",
        "  )\n",
        "\n",
        "SELECT \n",
        "cluster,\n",
        "ts AS ds,\n",
        "ROUND(AVG(kwhh),4) AS y,\n",
        "ROUND(SUM(kwhh), 4) AS total_kwhh,\n",
        "MAX(air_temperature) AS air_temperature,\n",
        "COUNT(DISTINCT lcl_id) AS households_num\n",
        "FROM cluster_data\n",
        "\n",
        "GROUP BY 1,2\n",
        "ORDER BY 1,2"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWg9iqUPvVB4",
        "colab_type": "text"
      },
      "source": [
        "##### format data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQaotWvpeDHx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "31235801-7e5c-4bf0-8ae8-e67824ea43bf"
      },
      "source": [
        "df_som_train['ds'] = df_som_train['ds'].dt.tz_localize(None) # remove timezones \n",
        "df_som_test['ds'] = df_som_test['ds'].dt.tz_localize(None) # remove timezones \n",
        "\n",
        "df_som_train.groupby('ds').mean().head(10), df_som_test.groupby('ds').mean().head(10)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(                            y  total_kwhh  air_temperature  households_num\n",
              " ds                                                                        \n",
              " 2012-11-01 00:00:00  0.214467   55.002556             11.8      297.777778\n",
              " 2012-11-01 00:30:00  0.229322   58.101111             11.8      297.777778\n",
              " 2012-11-01 01:00:00  0.199556   50.510444              8.8      297.777778\n",
              " 2012-11-01 01:30:00  0.179867   45.733556              8.8      297.777778\n",
              " 2012-11-01 02:00:00  0.160822   41.094000              8.5      297.333333\n",
              " 2012-11-01 02:30:00  0.144544   37.313667              8.5      297.666667\n",
              " 2012-11-01 03:00:00  0.134667   34.895222              7.8      297.777778\n",
              " 2012-11-01 03:30:00  0.129956   33.504222              7.8      297.777778\n",
              " 2012-11-01 04:00:00  0.127622   33.137667              7.1      297.777778\n",
              " 2012-11-01 04:30:00  0.129944   33.433333              7.1      297.777778,\n",
              "                             y  total_kwhh  air_temperature  households_num\n",
              " ds                                                                        \n",
              " 2012-11-01 00:00:00  0.203833   21.003444             11.8      111.000000\n",
              " 2012-11-01 00:30:00  0.224756   23.161111             11.8      111.111111\n",
              " 2012-11-01 01:00:00  0.200633   20.585222              8.8      111.111111\n",
              " 2012-11-01 01:30:00  0.176011   18.377222              8.8      111.111111\n",
              " 2012-11-01 02:00:00  0.158622   16.598778              8.5      111.111111\n",
              " 2012-11-01 02:30:00  0.146478   15.536111              8.5      111.111111\n",
              " 2012-11-01 03:00:00  0.135522   14.892778              7.8      111.111111\n",
              " 2012-11-01 03:30:00  0.127944   14.277444              7.8      111.111111\n",
              " 2012-11-01 04:00:00  0.125067   13.640444              7.1      111.111111\n",
              " 2012-11-01 04:30:00  0.126333   13.411000              7.1      111.111111)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxbwzjGRtxvT",
        "colab_type": "text"
      },
      "source": [
        "# **3Ô∏è‚É£ Training Models**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qXNrM1qgOdBx",
        "colab_type": "text"
      },
      "source": [
        "### **Training baseline forecast**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFPpZZmKPRZN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "f094895a-c347-4435-c1b0-e17861c82719"
      },
      "source": [
        "baseline_model = TrainProphet(\"2013-02-01\")\n",
        "baseline_model.fit(df_baseline_train)\n",
        "baseline_model.evaluate_test_global_mape(df_baseline_test)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:numexpr.utils:NumExpr defaulting to 2 threads.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Mean Absolute Percentage Error: 7.454017857142844\n",
            "Test Mean Absolute Percentage Error: 8.88\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<src.train_prophet.TrainProphet at 0x7ff51205b2e8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdJ-D9WwBs1s",
        "colab_type": "text"
      },
      "source": [
        "### **Training Agglo Clusters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13SAKoThmntS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_clusters(df_train, df_test, test_period=\"2013-02-01\"):\n",
        "    forecast_dict = {}\n",
        "    test_global_fc = pd.DataFrame()\n",
        "    train_global_fc = pd.DataFrame()\n",
        "    clusters = df_train.groupby('cluster').count().index.to_list()\n",
        "    for cluster in clusters:\n",
        "        cluster_dict = {} \n",
        "        print(f\"\\nTraining cluster: {cluster}\") \n",
        "        print(\"---------------------------\")\n",
        "        df_train_cluster = df_train.query(f\"cluster=='{cluster}'\").copy()\n",
        "        df_test_cluster = df_test.query(f\"cluster=='{cluster}'\").copy()\n",
        "        model = TrainProphet(test_period)\n",
        "        model.fit(df_train_cluster)\n",
        "        model.evaluate_test_global_mape(df_test_cluster)\n",
        "        cluster_dict['model'] = model \n",
        "        forecast_dict[f'cluster_{cluster}']=cluster_dict\n",
        "        test_global_fc = pd.concat([test_global_fc, model.test_forecast])\n",
        "\n",
        "        train_forecast = df_train[['cluster','ds','y']].copy()\n",
        "        train_forecast['max_households'] = df_train['households_num'].max()\n",
        "        train_forecast = train_forecast.merge(model.forecast[['ds', 'yhat']], left_on='ds', right_on='ds')\n",
        "        train_forecast['y_global'] = train_forecast['y'] * train_forecast['max_households']\n",
        "        train_forecast['yhat_global'] = train_forecast['yhat'] * train_forecast['max_households']\n",
        "        train_global_fc = pd.concat([train_global_fc, train_forecast])\n",
        "\n",
        "    return forecast_dict, test_global_fc, train_global_fc"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0e6aVOAn0LZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "outputId": "b4430443-cfe2-4269-ae71-4ba0d7250565"
      },
      "source": [
        "agglo_dict, agglo_global_test, agglo_global_train = train_clusters(df_agglo_train, df_agglo_test)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training cluster: 0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 6.713616071428577\n",
            "Test Mean Absolute Percentage Error: 72.09\n",
            "\n",
            "Training cluster: 1\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 10.567767857142881\n",
            "Test Mean Absolute Percentage Error: 68.99\n",
            "\n",
            "Training cluster: 2\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 93.82300595238095\n",
            "Test Mean Absolute Percentage Error: 74.17\n",
            "\n",
            "Training cluster: 3\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 9.677522321428553\n",
            "Test Mean Absolute Percentage Error: 376.27000000000004\n",
            "\n",
            "Training cluster: 4\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 9.18360863095236\n",
            "Test Mean Absolute Percentage Error: 11.4\n",
            "\n",
            "Training cluster: 5\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 1733264479330.5093\n",
            "Test Mean Absolute Percentage Error: 108.16\n",
            "\n",
            "Training cluster: 6\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 9.671875000000002\n",
            "Test Mean Absolute Percentage Error: 40.2\n",
            "\n",
            "Training cluster: 7\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 23.16162946428573\n",
            "Test Mean Absolute Percentage Error: 1159.44\n",
            "\n",
            "Training cluster: 8\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 89.07718750000006\n",
            "Test Mean Absolute Percentage Error: 333.97\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O3ikE9fh6dQn",
        "colab_type": "text"
      },
      "source": [
        "#####¬†Evaluate Test/ Train MAPE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-RsQRB2qIk2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "686ec1bd-6568-419f-a820-facf190bb89e"
      },
      "source": [
        "agglo_test = agglo_global_test.groupby('ds')[['households_num', 'y_global','yhat_global']].sum()\n",
        "agglo_test_global_mape = np.round(np.mean(np.abs(agglo_test['yhat_global']/agglo_test['y_global']-1)),4)*100\n",
        "print(f\"agglo_test_global_mape: {agglo_test_global_mape}\")"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "agglo_test_global_mape: 298.53000000000003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxfTlPXH4d3b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "705b2774-5704-4ad6-c7a1-a53d82f15231"
      },
      "source": [
        "agglo_train = agglo_global_train.groupby('ds')[['y_global','yhat_global']].sum()\n",
        "agglo_train_global_mape = np.round(np.mean(np.abs(agglo_train['yhat_global']/agglo_train['y_global']-1)),4)*100\n",
        "print(f\"agglo_train_global_mape: {agglo_train_global_mape}\")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "agglo_train_global_mape: 10.489999999999998\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGTDcnAZcUxk",
        "colab_type": "text"
      },
      "source": [
        "### **Training SOM Clusters**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5aw7zAQt47G",
        "colab_type": "text"
      },
      "source": [
        "##### Training Forecast"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k08MKMoPrqTq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 782
        },
        "outputId": "308c65e8-1bd4-44e7-95b5-1e3f3fae9d68"
      },
      "source": [
        "som_dict, som_global_test, som_global_train = train_clusters(df_som_train, df_som_test)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training cluster: 0\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 6.837886904761923\n",
            "Test Mean Absolute Percentage Error: 52.28\n",
            "\n",
            "Training cluster: 1\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 8.251034226190475\n",
            "Test Mean Absolute Percentage Error: 17.36\n",
            "\n",
            "Training cluster: 2\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 9.76328125000001\n",
            "Test Mean Absolute Percentage Error: 18.990000000000002\n",
            "\n",
            "Training cluster: 3\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 12.772157738095249\n",
            "Test Mean Absolute Percentage Error: 25.28\n",
            "\n",
            "Training cluster: 4\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 17.782291666666655\n",
            "Test Mean Absolute Percentage Error: 34.64\n",
            "\n",
            "Training cluster: 5\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 12.562418154761891\n",
            "Test Mean Absolute Percentage Error: 35.86\n",
            "\n",
            "Training cluster: 6\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 11.088578869047637\n",
            "Test Mean Absolute Percentage Error: 26.979999999999997\n",
            "\n",
            "Training cluster: 7\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 12.033802083333336\n",
            "Test Mean Absolute Percentage Error: 69.35\n",
            "\n",
            "Training cluster: 8\n",
            "---------------------------\n",
            "Training Mean Absolute Percentage Error: 8.298839285714285\n",
            "Test Mean Absolute Percentage Error: 189.97\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exsz0aGj6hq0",
        "colab_type": "text"
      },
      "source": [
        "##### Evaluate Test Train Mape"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7cjNcmG2ZLl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "af00a98f-ec76-47f7-cdd8-18040d70c2d3"
      },
      "source": [
        "som_test = som_global_test.groupby('ds')[['y_global','yhat_global']].sum()\n",
        "som_test_global_mape = np.round(np.mean(np.abs(som_test['yhat_global']/som_test['y_global']-1)),4)*100\n",
        "print(f\"som_test_global_mape: {som_test_global_mape}\")"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "som_test_global_mape: 8.41\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nH4wqQyS6DBO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "135fcd82-b543-464d-f26b-b840e28588b4"
      },
      "source": [
        "som_train = som_global_train.groupby('ds')[['y_global','yhat_global']].sum()\n",
        "som_train_global_mape = np.round(np.mean(np.abs(som_train['yhat_global']/som_train['y_global']-1)),4)*100\n",
        "print(f\"som_train_global_mape: {som_train_global_mape}\")"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "som_train_global_mape: 7.9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkSjdU-W9JYf",
        "colab_type": "text"
      },
      "source": [
        "# **4Ô∏è‚É£ Save Predictions to BQ**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYboWHXCDcuk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas_gbq\n",
        "def output_to_bq(forecast, table_id, project_id='machine-learning-msc'):\n",
        "    pandas_gbq.to_gbq(forecast, table_id, project_id=project_id)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwr-FWn5ESbt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "99e6fc1c-92a5-4744-c16b-569962c7b56c"
      },
      "source": [
        "output_to_bq(baseline_model.forecast[['ds','yhat']], table_id='forecasting_20200719.baseline_train_prediction')\n",
        "output_to_bq(baseline_model.test_forecast[['ds','yhat']], table_id='forecasting_20200719.baseline_test_prediction')"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5760 out of 5760 rows loaded.\n",
            "1it [00:03,  3.64s/it]\n",
            "1344 out of 1344 rows loaded.\n",
            "1it [00:04,  4.79s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IhmofN4Ek5g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "da646092-7bd4-4b0e-cc23-942295353c53"
      },
      "source": [
        "output_to_bq(agglo_global_train[['ds','yhat','cluster']], table_id='forecasting_20200719.agglo_train_prediction')\n",
        "output_to_bq(agglo_global_test[['ds','yhat','cluster']], table_id='forecasting_20200719.agglo_test_prediction')"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "466560 out of 466560 rows loaded.\n",
            "1it [00:44, 44.47s/it]\n",
            "12096 out of 12096 rows loaded.\n",
            "1it [00:06,  6.92s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAT19CJIEqo7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "514b1cac-0d19-40d8-ef24-50fa12e58152"
      },
      "source": [
        "output_to_bq(som_global_train[['ds','yhat','cluster']], table_id='forecasting_20200719.som_train_prediction')\n",
        "output_to_bq(som_global_test[['ds','yhat','cluster']], table_id='forecasting_20200719.som_test_prediction')"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "466560 out of 466560 rows loaded.\n",
            "1it [00:26, 26.11s/it]\n",
            "12096 out of 12096 rows loaded.\n",
            "1it [00:03,  3.34s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}